{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import autoaugment\n",
    "from PIL import Image\n",
    "import time\n",
    "import wandb\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. Data Loading and Preprocessing with configurable augmentation\n",
    "def get_data_loaders(batch_size=128, num_workers=0, config=None):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2470, 0.2435, 0.2616]\n",
    "    )\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    \n",
    "    # 使用 wandb config 来控制增强参数\n",
    "    train_transform_list = [transforms.ToTensor(), normalize]\n",
    "    if config.random_crop:\n",
    "        train_transform_list.insert(0, transforms.RandomCrop(32, padding=4))\n",
    "    if config.horizontal_flip:\n",
    "        train_transform_list.insert(0, transforms.RandomHorizontalFlip())\n",
    "    train_transform_list.insert(0, transforms.ColorJitter(\n",
    "        brightness=config.color_jitter_brightness,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.1\n",
    "    ))\n",
    "    train_transform_list.insert(0, autoaugment.AutoAugment(policy=autoaugment.AutoAugmentPolicy.CIFAR10))\n",
    "    train_transform_list.append(transforms.RandomErasing(\n",
    "        p=config.random_erasing_prob,\n",
    "        scale=(0.02, 0.33),\n",
    "        ratio=(0.3, 3.3),\n",
    "        value=0\n",
    "    ))\n",
    "    \n",
    "    train_transform = transforms.Compose(train_transform_list)\n",
    "    \n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', \n",
    "        train=True,\n",
    "        download=True, \n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', \n",
    "        train=False,\n",
    "        download=True, \n",
    "        transform=test_transform\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Definition (unchanged)\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    return total_params, trainable_params\n",
    "\n",
    "def get_resnet18_model(num_classes=10, pretrained=False):\n",
    "    model = models.resnet18(weights=weights)\n",
    "    model.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.bn1 = nn.BatchNorm2d(32)\n",
    "    model.maxpool = nn.Identity()\n",
    "    \n",
    "    model.layer1[0].conv1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.layer1[0].conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.layer1[0].bn1 = nn.BatchNorm2d(32)\n",
    "    model.layer1[0].bn2 = nn.BatchNorm2d(32)\n",
    "    model.layer1[1].conv1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.layer1[1].conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.layer1[1].bn1 = nn.BatchNorm2d(32)\n",
    "    model.layer1[1].bn2 = nn.BatchNorm2d(32)\n",
    "    \n",
    "    model.layer2[0].conv1 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "    model.layer2[0].conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.layer2[0].bn1 = nn.BatchNorm2d(64)\n",
    "    model.layer2[0].bn2 = nn.BatchNorm2d(64)\n",
    "    model.layer2[0].downsample[0] = nn.Conv2d(32, 64, kernel_size=1, stride=2, bias=False)\n",
    "    model.layer2[0].downsample[1] = nn.BatchNorm2d(64)\n",
    "    model.layer2[1].conv1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.layer2[1].conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.layer2[1].bn1 = nn.BatchNorm2d(64)\n",
    "    model.layer2[1].bn2 = nn.BatchNorm2d(64)\n",
    "    \n",
    "    model.layer3[0].conv1 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "    model.layer3[0].conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.layer3[0].bn1 = nn.BatchNorm2d(128)\n",
    "    model.layer3[0].bn2 = nn.BatchNorm2d(128)\n",
    "    model.layer3[0].downsample[0] = nn.Conv2d(64, 128, kernel_size=1, stride=2, bias=False)\n",
    "    model.layer3[0].downsample[1] = nn.BatchNorm2d(128)\n",
    "    model.layer3[1].conv1 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.layer3[1].conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.layer3[1].bn1 = nn.BatchNorm2d(128)\n",
    "    model.layer3[1].bn2 = nn.BatchNorm2d(128)\n",
    "    \n",
    "    model.layer4[0].conv1 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "    model.layer4[0].conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.layer4[0].bn1 = nn.BatchNorm2d(256)\n",
    "    model.layer4[0].bn2 = nn.BatchNorm2d(256)\n",
    "    model.layer4[0].downsample[0] = nn.Conv2d(128, 256, kernel_size=1, stride=2, bias=False)\n",
    "    model.layer4[0].downsample[1] = nn.BatchNorm2d(256)\n",
    "    model.layer4[1].conv1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.layer4[1].conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.layer4[1].bn1 = nn.BatchNorm2d(256)\n",
    "    model.layer4[1].bn2 = nn.BatchNorm2d(256)\n",
    "    \n",
    "    model.fc = nn.Linear(256, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Mixup Function (unchanged)\n",
    "def mixup_data(batch, targets, alpha=1.0, device='cpu'):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = batch.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * batch + (1 - lam) * batch[index]\n",
    "    y_a, y_b = targets, targets[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training and Evaluation Functions with wandb logging\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, use_mixup=False, mixup_alpha=1.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        if use_mixup:\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha=mixup_alpha, device=device)\n",
    "            outputs = model(inputs)\n",
    "            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if not use_mixup:\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    if not use_mixup:\n",
    "        epoch_acc = 100.0 * correct / total\n",
    "        wandb.log({\"train_loss\": epoch_loss, \"train_accuracy\": epoch_acc, \"epoch\": epoch})\n",
    "        print(f'Epoch: {epoch}, Train Loss: {epoch_loss:.3f}, Train Acc: {epoch_acc:.2f}%')\n",
    "        return epoch_loss, epoch_acc\n",
    "    else:\n",
    "        wandb.log({\"train_loss\": epoch_loss, \"epoch\": epoch})\n",
    "        print(f'Epoch: {epoch}, Train Loss: {epoch_loss:.3f}')\n",
    "        return epoch_loss, None\n",
    "    \n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100.0 * correct / total\n",
    "    wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_acc})\n",
    "    print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5. Main Training Function with MPS Support\n",
    "# def train(config=None):\n",
    "#     with wandb.init(config=config):\n",
    "#         config = wandb.config\n",
    "        \n",
    "#         # 设备检测：优先 MPS，然后 CUDA，最后 CPU\n",
    "#         if torch.backends.mps.is_available():\n",
    "#             device = torch.device(\"mps\")\n",
    "#             print(f\"Using Apple MPS for acceleration\")\n",
    "#         elif torch.cuda.is_available():\n",
    "#             device = torch.device(\"cuda\")\n",
    "#             print(f\"Using CUDA for acceleration\")\n",
    "#         else:\n",
    "#             device = torch.device(\"cpu\")\n",
    "#             print(f\"Using CPU (no GPU/MPS available)\")\n",
    "        \n",
    "#         # 获取数据加载器\n",
    "#         train_loader, test_loader = get_data_loaders(batch_size=128, config=config)\n",
    "        \n",
    "#         # 初始化模型并移动到指定设备\n",
    "#         model = get_resnet18_model(num_classes=10).to(device)\n",
    "#         count_parameters(model)\n",
    "        \n",
    "#         # 定义损失函数和优化器\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "        \n",
    "#         # 获取训练的总轮数\n",
    "#         num_epochs = config.num_epochs if hasattr(config, 'num_epochs') else 20\n",
    "        \n",
    "#         # 调整学习率调度器的最大周期为训练轮数\n",
    "#         scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "        \n",
    "#         # 训练循环\n",
    "#         for epoch in range(num_epochs):\n",
    "#             train_loss, _ = train_one_epoch(\n",
    "#                 model, train_loader, criterion, optimizer, device, epoch+1,\n",
    "#                 use_mixup=True, mixup_alpha=config.mixup_alpha\n",
    "#             )\n",
    "#             test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "#             scheduler.step()\n",
    "\n",
    "# 5. Main Training Function with MPS Support\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Get the unique run ID to use in filenames\n",
    "        run_id = wandb.run.id\n",
    "        \n",
    "        # 设备检测：优先 MPS，然后 CUDA，最后 CPU\n",
    "        if torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\")\n",
    "            print(f\"Using Apple MPS for acceleration\")\n",
    "        elif torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(f\"Using CUDA for acceleration\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(f\"Using CPU (no GPU/MPS available)\")\n",
    "        \n",
    "        # 获取数据加载器\n",
    "        train_loader, test_loader = get_data_loaders(batch_size=128, config=config)\n",
    "        \n",
    "        # 初始化模型并移动到指定设备\n",
    "        model = get_resnet18_model(num_classes=10).to(device)\n",
    "        count_parameters(model)\n",
    "        \n",
    "        # 定义损失函数和优化器\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "        \n",
    "        # 获取训练的总轮数\n",
    "        num_epochs = config.num_epochs if hasattr(config, 'num_epochs') else 20\n",
    "        \n",
    "        # 调整学习率调度器的最大周期为训练轮数\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "        \n",
    "        # 跟踪最佳模型\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        # Create model filenames with run ID\n",
    "        best_model_path = f'best_model_{run_id}.pth'\n",
    "        final_model_path = f'final_model_{run_id}.pth'\n",
    "        \n",
    "        # 训练循环\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, _ = train_one_epoch(\n",
    "                model, train_loader, criterion, optimizer, device, epoch+1,\n",
    "                use_mixup=True, mixup_alpha=config.mixup_alpha\n",
    "            )\n",
    "            test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "            scheduler.step()\n",
    "            \n",
    "            # 保存最佳模型\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                # 保存到本地\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                # 保存到wandb\n",
    "                wandb.save(best_model_path)\n",
    "                \n",
    "                # Also log the best accuracy to wandb config for easy retrieval\n",
    "                wandb.run.summary['best_accuracy'] = best_acc\n",
    "        \n",
    "        # 保存最终模型\n",
    "        torch.save(model.state_dict(), final_model_path)\n",
    "        wandb.save(final_model_path)\n",
    "        \n",
    "        # Log final test accuracy\n",
    "        wandb.run.summary['final_accuracy'] = test_acc\n",
    "        \n",
    "        # Also save a record of which model is best for this run\n",
    "        with open(f'model_info_{run_id}.txt', 'w') as f:\n",
    "            f.write(f\"Best model: {best_model_path}, Accuracy: {best_acc:.2f}%\\n\")\n",
    "            f.write(f\"Final model: {final_model_path}, Accuracy: {test_acc:.2f}%\\n\")\n",
    "            f.write(\"\\nRun Configuration:\\n\")\n",
    "            for key, value in config._items.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        wandb.save(f'model_info_{run_id}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: knffhnpf\n",
      "Sweep URL: https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f3szj3jc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcolor_jitter_brightness: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thorizontal_flip: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmixup_alpha: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 150\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_crop: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_erasing_prob: 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mprime-sweep-2\u001b[0m at: \u001b[34mhttps://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/riktn1e3\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250306_201245-riktn1e3/logs\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/JL/Desktop/DL_Proj/wandb/run-20250307_111915-f3szj3jc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/f3szj3jc' target=\"_blank\">sparkling-sweep-1</a></strong> to <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/f3szj3jc' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/f3szj3jc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS for acceleration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JL/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/JL/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2,797,610\n",
      "Trainable parameters: 2,797,610\n",
      "Epoch: 1, Train Loss: 2.086\n",
      "Test Loss: 1.546, Test Acc: 42.53%\n",
      "Epoch: 2, Train Loss: 1.768\n",
      "Test Loss: 1.702, Test Acc: 44.99%\n",
      "Epoch: 3, Train Loss: 1.588\n",
      "Test Loss: 1.052, Test Acc: 61.57%\n",
      "Epoch: 4, Train Loss: 1.420\n",
      "Test Loss: 1.135, Test Acc: 59.24%\n",
      "Epoch: 5, Train Loss: 1.366\n",
      "Test Loss: 0.975, Test Acc: 66.83%\n",
      "Epoch: 6, Train Loss: 1.316\n",
      "Test Loss: 0.848, Test Acc: 71.42%\n",
      "Epoch: 7, Train Loss: 1.245\n",
      "Test Loss: 0.763, Test Acc: 73.78%\n",
      "Epoch: 8, Train Loss: 1.213\n",
      "Test Loss: 0.854, Test Acc: 70.75%\n",
      "Epoch: 9, Train Loss: 1.232\n",
      "Test Loss: 0.733, Test Acc: 76.60%\n",
      "Epoch: 10, Train Loss: 1.185\n",
      "Test Loss: 0.770, Test Acc: 74.01%\n",
      "Epoch: 11, Train Loss: 1.143\n",
      "Test Loss: 0.733, Test Acc: 75.39%\n",
      "Epoch: 12, Train Loss: 1.132\n",
      "Test Loss: 0.841, Test Acc: 72.04%\n",
      "Epoch: 13, Train Loss: 1.157\n",
      "Test Loss: 0.854, Test Acc: 71.22%\n",
      "Epoch: 14, Train Loss: 1.155\n",
      "Test Loss: 0.660, Test Acc: 78.34%\n",
      "Epoch: 15, Train Loss: 1.122\n",
      "Test Loss: 0.722, Test Acc: 76.90%\n",
      "Epoch: 16, Train Loss: 1.154\n",
      "Test Loss: 0.802, Test Acc: 72.28%\n",
      "Epoch: 17, Train Loss: 1.147\n",
      "Test Loss: 0.725, Test Acc: 77.07%\n",
      "Epoch: 18, Train Loss: 1.130\n",
      "Test Loss: 0.623, Test Acc: 80.44%\n",
      "Epoch: 19, Train Loss: 1.099\n",
      "Test Loss: 0.803, Test Acc: 73.01%\n",
      "Epoch: 20, Train Loss: 1.117\n",
      "Test Loss: 0.634, Test Acc: 78.73%\n",
      "Epoch: 21, Train Loss: 1.099\n",
      "Test Loss: 0.677, Test Acc: 76.95%\n",
      "Epoch: 22, Train Loss: 1.102\n",
      "Test Loss: 0.672, Test Acc: 77.76%\n",
      "Epoch: 23, Train Loss: 1.059\n",
      "Test Loss: 0.631, Test Acc: 79.34%\n",
      "Epoch: 24, Train Loss: 1.069\n",
      "Test Loss: 0.718, Test Acc: 76.73%\n",
      "Epoch: 25, Train Loss: 1.065\n",
      "Test Loss: 0.565, Test Acc: 81.24%\n",
      "Epoch: 26, Train Loss: 1.057\n",
      "Test Loss: 0.616, Test Acc: 79.26%\n",
      "Epoch: 27, Train Loss: 1.059\n",
      "Test Loss: 0.734, Test Acc: 76.48%\n",
      "Epoch: 28, Train Loss: 1.058\n",
      "Test Loss: 0.614, Test Acc: 81.42%\n",
      "Epoch: 29, Train Loss: 1.070\n",
      "Test Loss: 0.675, Test Acc: 76.71%\n",
      "Epoch: 30, Train Loss: 1.070\n",
      "Test Loss: 0.636, Test Acc: 80.43%\n",
      "Epoch: 31, Train Loss: 1.065\n",
      "Test Loss: 0.618, Test Acc: 79.53%\n",
      "Epoch: 32, Train Loss: 1.078\n",
      "Test Loss: 0.574, Test Acc: 80.82%\n",
      "Epoch: 33, Train Loss: 1.091\n",
      "Test Loss: 0.646, Test Acc: 80.35%\n",
      "Epoch: 34, Train Loss: 1.046\n",
      "Test Loss: 0.650, Test Acc: 78.66%\n",
      "Epoch: 35, Train Loss: 1.045\n",
      "Test Loss: 0.652, Test Acc: 78.42%\n",
      "Epoch: 36, Train Loss: 1.011\n",
      "Test Loss: 0.638, Test Acc: 79.69%\n",
      "Epoch: 37, Train Loss: 1.086\n",
      "Test Loss: 0.573, Test Acc: 82.49%\n",
      "Epoch: 38, Train Loss: 1.033\n",
      "Test Loss: 0.605, Test Acc: 80.07%\n",
      "Epoch: 39, Train Loss: 1.050\n",
      "Test Loss: 0.561, Test Acc: 82.11%\n",
      "Epoch: 40, Train Loss: 1.046\n",
      "Test Loss: 0.668, Test Acc: 80.80%\n",
      "Epoch: 41, Train Loss: 1.031\n",
      "Test Loss: 0.664, Test Acc: 77.97%\n",
      "Epoch: 42, Train Loss: 1.019\n",
      "Test Loss: 0.629, Test Acc: 79.49%\n",
      "Epoch: 43, Train Loss: 0.992\n",
      "Test Loss: 0.637, Test Acc: 78.87%\n",
      "Epoch: 44, Train Loss: 1.028\n",
      "Test Loss: 0.703, Test Acc: 77.03%\n",
      "Epoch: 45, Train Loss: 0.998\n",
      "Test Loss: 0.557, Test Acc: 82.07%\n",
      "Epoch: 46, Train Loss: 1.014\n",
      "Test Loss: 0.535, Test Acc: 82.66%\n",
      "Epoch: 47, Train Loss: 1.020\n",
      "Test Loss: 0.579, Test Acc: 80.46%\n",
      "Epoch: 48, Train Loss: 1.001\n",
      "Test Loss: 0.648, Test Acc: 78.84%\n",
      "Epoch: 49, Train Loss: 1.029\n",
      "Test Loss: 0.529, Test Acc: 83.05%\n",
      "Epoch: 50, Train Loss: 0.999\n",
      "Test Loss: 0.549, Test Acc: 81.78%\n",
      "Epoch: 51, Train Loss: 0.984\n",
      "Test Loss: 0.566, Test Acc: 83.07%\n",
      "Epoch: 52, Train Loss: 1.042\n",
      "Test Loss: 0.532, Test Acc: 84.58%\n",
      "Epoch: 53, Train Loss: 1.005\n",
      "Test Loss: 0.548, Test Acc: 83.66%\n",
      "Epoch: 54, Train Loss: 1.017\n",
      "Test Loss: 0.547, Test Acc: 82.19%\n",
      "Epoch: 55, Train Loss: 0.996\n",
      "Test Loss: 0.727, Test Acc: 78.11%\n",
      "Epoch: 56, Train Loss: 0.970\n",
      "Test Loss: 0.585, Test Acc: 82.05%\n",
      "Epoch: 57, Train Loss: 0.973\n",
      "Test Loss: 0.471, Test Acc: 85.06%\n",
      "Epoch: 58, Train Loss: 0.987\n",
      "Test Loss: 0.515, Test Acc: 84.10%\n",
      "Epoch: 59, Train Loss: 0.999\n",
      "Test Loss: 0.558, Test Acc: 81.47%\n",
      "Epoch: 60, Train Loss: 0.970\n",
      "Test Loss: 0.508, Test Acc: 83.83%\n",
      "Epoch: 61, Train Loss: 1.007\n",
      "Test Loss: 0.578, Test Acc: 82.40%\n",
      "Epoch: 62, Train Loss: 1.006\n",
      "Test Loss: 0.681, Test Acc: 77.32%\n",
      "Epoch: 63, Train Loss: 0.974\n",
      "Test Loss: 0.561, Test Acc: 81.34%\n",
      "Epoch: 64, Train Loss: 0.931\n",
      "Test Loss: 0.476, Test Acc: 84.32%\n",
      "Epoch: 65, Train Loss: 0.937\n",
      "Test Loss: 0.534, Test Acc: 82.45%\n",
      "Epoch: 66, Train Loss: 0.964\n",
      "Test Loss: 0.499, Test Acc: 84.92%\n",
      "Epoch: 67, Train Loss: 0.962\n",
      "Test Loss: 0.718, Test Acc: 75.23%\n",
      "Epoch: 68, Train Loss: 0.922\n",
      "Test Loss: 0.505, Test Acc: 83.15%\n",
      "Epoch: 69, Train Loss: 0.952\n",
      "Test Loss: 0.612, Test Acc: 80.60%\n",
      "Epoch: 70, Train Loss: 0.977\n",
      "Test Loss: 0.541, Test Acc: 84.12%\n",
      "Epoch: 71, Train Loss: 0.925\n",
      "Test Loss: 0.455, Test Acc: 85.67%\n",
      "Epoch: 72, Train Loss: 0.936\n",
      "Test Loss: 0.481, Test Acc: 85.43%\n",
      "Epoch: 73, Train Loss: 0.935\n",
      "Test Loss: 0.581, Test Acc: 82.89%\n",
      "Epoch: 74, Train Loss: 0.940\n",
      "Test Loss: 0.457, Test Acc: 86.03%\n",
      "Epoch: 75, Train Loss: 0.961\n",
      "Test Loss: 0.651, Test Acc: 79.07%\n",
      "Epoch: 76, Train Loss: 0.917\n",
      "Test Loss: 0.460, Test Acc: 85.37%\n",
      "Epoch: 77, Train Loss: 0.938\n",
      "Test Loss: 0.470, Test Acc: 84.71%\n",
      "Epoch: 78, Train Loss: 0.917\n",
      "Test Loss: 0.466, Test Acc: 85.19%\n",
      "Epoch: 79, Train Loss: 0.964\n",
      "Test Loss: 0.566, Test Acc: 83.59%\n",
      "Epoch: 80, Train Loss: 0.886\n",
      "Test Loss: 0.505, Test Acc: 85.03%\n",
      "Epoch: 81, Train Loss: 0.936\n",
      "Test Loss: 0.509, Test Acc: 86.24%\n",
      "Epoch: 82, Train Loss: 0.889\n",
      "Test Loss: 0.439, Test Acc: 86.57%\n",
      "Epoch: 83, Train Loss: 0.923\n",
      "Test Loss: 0.463, Test Acc: 85.84%\n",
      "Epoch: 84, Train Loss: 0.899\n",
      "Test Loss: 0.501, Test Acc: 85.08%\n",
      "Epoch: 85, Train Loss: 0.905\n",
      "Test Loss: 0.503, Test Acc: 84.75%\n",
      "Epoch: 86, Train Loss: 0.908\n",
      "Test Loss: 0.471, Test Acc: 85.72%\n",
      "Epoch: 87, Train Loss: 0.911\n",
      "Test Loss: 0.437, Test Acc: 87.14%\n",
      "Epoch: 88, Train Loss: 0.850\n",
      "Test Loss: 0.479, Test Acc: 84.38%\n",
      "Epoch: 89, Train Loss: 0.866\n",
      "Test Loss: 0.396, Test Acc: 88.27%\n",
      "Epoch: 90, Train Loss: 0.856\n",
      "Test Loss: 0.379, Test Acc: 87.20%\n",
      "Epoch: 91, Train Loss: 0.848\n",
      "Test Loss: 0.458, Test Acc: 85.21%\n",
      "Epoch: 92, Train Loss: 0.854\n",
      "Test Loss: 0.439, Test Acc: 88.23%\n",
      "Epoch: 93, Train Loss: 0.870\n",
      "Test Loss: 0.393, Test Acc: 87.33%\n",
      "Epoch: 94, Train Loss: 0.863\n",
      "Test Loss: 0.392, Test Acc: 88.00%\n",
      "Epoch: 95, Train Loss: 0.843\n",
      "Test Loss: 0.409, Test Acc: 87.15%\n",
      "Epoch: 96, Train Loss: 0.851\n",
      "Test Loss: 0.406, Test Acc: 88.31%\n",
      "Epoch: 97, Train Loss: 0.849\n",
      "Test Loss: 0.405, Test Acc: 88.20%\n",
      "Epoch: 98, Train Loss: 0.840\n",
      "Test Loss: 0.397, Test Acc: 87.85%\n",
      "Epoch: 99, Train Loss: 0.849\n",
      "Test Loss: 0.375, Test Acc: 88.99%\n",
      "Epoch: 100, Train Loss: 0.780\n",
      "Test Loss: 0.375, Test Acc: 88.08%\n",
      "Epoch: 101, Train Loss: 0.836\n",
      "Test Loss: 0.454, Test Acc: 88.51%\n",
      "Epoch: 102, Train Loss: 0.832\n",
      "Test Loss: 0.378, Test Acc: 89.16%\n",
      "Epoch: 103, Train Loss: 0.813\n",
      "Test Loss: 0.363, Test Acc: 89.24%\n",
      "Epoch: 104, Train Loss: 0.816\n",
      "Test Loss: 0.358, Test Acc: 89.49%\n",
      "Epoch: 105, Train Loss: 0.800\n",
      "Test Loss: 0.342, Test Acc: 90.06%\n",
      "Epoch: 106, Train Loss: 0.785\n",
      "Test Loss: 0.355, Test Acc: 90.07%\n",
      "Epoch: 107, Train Loss: 0.755\n",
      "Test Loss: 0.393, Test Acc: 87.95%\n",
      "Epoch: 108, Train Loss: 0.792\n",
      "Test Loss: 0.325, Test Acc: 90.00%\n",
      "Epoch: 109, Train Loss: 0.818\n",
      "Test Loss: 0.340, Test Acc: 90.05%\n",
      "Epoch: 110, Train Loss: 0.758\n",
      "Test Loss: 0.290, Test Acc: 90.58%\n",
      "Epoch: 111, Train Loss: 0.831\n",
      "Test Loss: 0.364, Test Acc: 90.05%\n",
      "Epoch: 112, Train Loss: 0.780\n",
      "Test Loss: 0.336, Test Acc: 89.40%\n",
      "Epoch: 113, Train Loss: 0.758\n",
      "Test Loss: 0.303, Test Acc: 90.87%\n",
      "Epoch: 114, Train Loss: 0.736\n",
      "Test Loss: 0.325, Test Acc: 90.69%\n",
      "Epoch: 115, Train Loss: 0.705\n",
      "Test Loss: 0.291, Test Acc: 91.81%\n",
      "Epoch: 116, Train Loss: 0.778\n",
      "Test Loss: 0.332, Test Acc: 90.71%\n",
      "Epoch: 117, Train Loss: 0.723\n",
      "Test Loss: 0.280, Test Acc: 92.05%\n",
      "Epoch: 118, Train Loss: 0.756\n",
      "Test Loss: 0.327, Test Acc: 90.14%\n",
      "Epoch: 119, Train Loss: 0.745\n",
      "Test Loss: 0.317, Test Acc: 91.19%\n",
      "Epoch: 120, Train Loss: 0.718\n",
      "Test Loss: 0.261, Test Acc: 92.36%\n",
      "Epoch: 121, Train Loss: 0.725\n",
      "Test Loss: 0.258, Test Acc: 92.42%\n",
      "Epoch: 122, Train Loss: 0.753\n",
      "Test Loss: 0.304, Test Acc: 91.84%\n",
      "Epoch: 123, Train Loss: 0.706\n",
      "Test Loss: 0.267, Test Acc: 92.19%\n",
      "Epoch: 124, Train Loss: 0.706\n",
      "Test Loss: 0.256, Test Acc: 92.73%\n",
      "Epoch: 125, Train Loss: 0.694\n",
      "Test Loss: 0.237, Test Acc: 92.62%\n",
      "Epoch: 126, Train Loss: 0.661\n",
      "Test Loss: 0.247, Test Acc: 93.11%\n",
      "Epoch: 127, Train Loss: 0.696\n",
      "Test Loss: 0.244, Test Acc: 93.50%\n",
      "Epoch: 128, Train Loss: 0.651\n",
      "Test Loss: 0.227, Test Acc: 93.14%\n",
      "Epoch: 129, Train Loss: 0.706\n",
      "Test Loss: 0.253, Test Acc: 93.00%\n",
      "Epoch: 130, Train Loss: 0.691\n",
      "Test Loss: 0.221, Test Acc: 93.44%\n",
      "Epoch: 131, Train Loss: 0.681\n",
      "Test Loss: 0.247, Test Acc: 93.59%\n",
      "Epoch: 132, Train Loss: 0.625\n",
      "Test Loss: 0.219, Test Acc: 93.46%\n",
      "Epoch: 133, Train Loss: 0.632\n",
      "Test Loss: 0.213, Test Acc: 94.03%\n",
      "Epoch: 134, Train Loss: 0.631\n",
      "Test Loss: 0.212, Test Acc: 93.88%\n",
      "Epoch: 135, Train Loss: 0.659\n",
      "Test Loss: 0.208, Test Acc: 94.16%\n",
      "Epoch: 136, Train Loss: 0.653\n",
      "Test Loss: 0.244, Test Acc: 94.14%\n",
      "Epoch: 137, Train Loss: 0.638\n",
      "Test Loss: 0.213, Test Acc: 94.04%\n",
      "Epoch: 138, Train Loss: 0.624\n",
      "Test Loss: 0.199, Test Acc: 94.39%\n",
      "Epoch: 139, Train Loss: 0.647\n",
      "Test Loss: 0.253, Test Acc: 94.37%\n",
      "Epoch: 140, Train Loss: 0.660\n",
      "Test Loss: 0.196, Test Acc: 94.53%\n",
      "Epoch: 141, Train Loss: 0.627\n",
      "Test Loss: 0.204, Test Acc: 94.53%\n",
      "Epoch: 142, Train Loss: 0.614\n",
      "Test Loss: 0.196, Test Acc: 94.51%\n",
      "Epoch: 143, Train Loss: 0.628\n",
      "Test Loss: 0.196, Test Acc: 94.49%\n",
      "Epoch: 144, Train Loss: 0.618\n",
      "Test Loss: 0.202, Test Acc: 94.60%\n",
      "Epoch: 145, Train Loss: 0.626\n",
      "Test Loss: 0.198, Test Acc: 94.62%\n",
      "Epoch: 146, Train Loss: 0.600\n",
      "Test Loss: 0.219, Test Acc: 94.49%\n",
      "Epoch: 147, Train Loss: 0.638\n",
      "Test Loss: 0.210, Test Acc: 94.50%\n",
      "Epoch: 148, Train Loss: 0.639\n",
      "Test Loss: 0.227, Test Acc: 94.64%\n",
      "Epoch: 149, Train Loss: 0.614\n",
      "Test Loss: 0.186, Test Acc: 94.74%\n",
      "Epoch: 150, Train Loss: 0.635\n",
      "Test Loss: 0.212, Test Acc: 94.52%\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>test_accuracy</td><td>▂▁▃▃▃▃▄▃▄▄▃▄▃▃▅▂▅▅▅▅▅▆▆▆▇▇▆▇▇▇██████████</td></tr><tr><td>test_loss</td><td>█▅▄▄▃▃▃▃▃▃▃▃▃▂▂▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_accuracy</td><td>94.74</td></tr><tr><td>epoch</td><td>150</td></tr><tr><td>final_accuracy</td><td>94.52</td></tr><tr><td>test_accuracy</td><td>94.52</td></tr><tr><td>test_loss</td><td>0.21232</td></tr><tr><td>train_loss</td><td>0.63485</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sparkling-sweep-1</strong> at: <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/f3szj3jc' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/f3szj3jc</a><br> View project at: <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250307_111915-f3szj3jc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: i8krnwwp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcolor_jitter_brightness: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thorizontal_flip: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmixup_alpha: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 150\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_crop: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_erasing_prob: 0.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/JL/Desktop/DL_Proj/wandb/run-20250307_123139-i8krnwwp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/i8krnwwp' target=\"_blank\">visionary-sweep-2</a></strong> to <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/i8krnwwp' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/i8krnwwp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS for acceleration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JL/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/JL/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2,797,610\n",
      "Trainable parameters: 2,797,610\n",
      "Epoch: 1, Train Loss: 2.116\n",
      "Test Loss: 1.726, Test Acc: 36.49%\n",
      "Epoch: 2, Train Loss: 1.898\n",
      "Test Loss: 1.382, Test Acc: 49.09%\n",
      "Epoch: 3, Train Loss: 1.735\n",
      "Test Loss: 1.245, Test Acc: 55.50%\n",
      "Epoch: 4, Train Loss: 1.592\n",
      "Test Loss: 1.125, Test Acc: 60.18%\n",
      "Epoch: 5, Train Loss: 1.488\n",
      "Test Loss: 1.057, Test Acc: 63.19%\n",
      "Epoch: 6, Train Loss: 1.392\n",
      "Test Loss: 0.915, Test Acc: 68.63%\n",
      "Epoch: 7, Train Loss: 1.352\n",
      "Test Loss: 1.033, Test Acc: 63.98%\n",
      "Epoch: 8, Train Loss: 1.313\n",
      "Test Loss: 0.949, Test Acc: 68.06%\n",
      "Epoch: 9, Train Loss: 1.262\n",
      "Test Loss: 0.733, Test Acc: 75.75%\n",
      "Epoch: 10, Train Loss: 1.272\n",
      "Test Loss: 0.922, Test Acc: 69.09%\n",
      "Epoch: 11, Train Loss: 1.262\n",
      "Test Loss: 0.832, Test Acc: 73.04%\n",
      "Epoch: 12, Train Loss: 1.242\n",
      "Test Loss: 0.655, Test Acc: 78.91%\n",
      "Epoch: 13, Train Loss: 1.193\n",
      "Test Loss: 0.730, Test Acc: 75.20%\n",
      "Epoch: 14, Train Loss: 1.209\n",
      "Test Loss: 0.739, Test Acc: 75.63%\n",
      "Epoch: 15, Train Loss: 1.209\n",
      "Test Loss: 0.908, Test Acc: 70.27%\n",
      "Epoch: 16, Train Loss: 1.189\n",
      "Test Loss: 0.616, Test Acc: 79.65%\n",
      "Epoch: 17, Train Loss: 1.149\n",
      "Test Loss: 0.698, Test Acc: 77.09%\n",
      "Epoch: 18, Train Loss: 1.166\n",
      "Test Loss: 0.641, Test Acc: 79.12%\n",
      "Epoch: 19, Train Loss: 1.165\n",
      "Test Loss: 0.809, Test Acc: 75.08%\n",
      "Epoch: 20, Train Loss: 1.133\n",
      "Test Loss: 0.757, Test Acc: 76.06%\n",
      "Epoch: 21, Train Loss: 1.117\n",
      "Test Loss: 0.692, Test Acc: 77.26%\n",
      "Epoch: 22, Train Loss: 1.145\n",
      "Test Loss: 0.623, Test Acc: 80.40%\n",
      "Epoch: 23, Train Loss: 1.112\n",
      "Test Loss: 0.681, Test Acc: 77.83%\n",
      "Epoch: 24, Train Loss: 1.119\n",
      "Test Loss: 0.704, Test Acc: 78.31%\n",
      "Epoch: 25, Train Loss: 1.136\n",
      "Test Loss: 0.671, Test Acc: 78.17%\n",
      "Epoch: 26, Train Loss: 1.098\n",
      "Test Loss: 0.642, Test Acc: 80.43%\n",
      "Epoch: 27, Train Loss: 1.131\n",
      "Test Loss: 0.667, Test Acc: 78.97%\n",
      "Epoch: 28, Train Loss: 1.112\n",
      "Test Loss: 0.717, Test Acc: 77.29%\n",
      "Epoch: 29, Train Loss: 1.101\n",
      "Test Loss: 0.567, Test Acc: 81.51%\n",
      "Epoch: 30, Train Loss: 1.068\n",
      "Test Loss: 0.675, Test Acc: 77.50%\n",
      "Epoch: 31, Train Loss: 1.071\n",
      "Test Loss: 0.868, Test Acc: 69.69%\n",
      "Epoch: 32, Train Loss: 1.092\n",
      "Test Loss: 0.569, Test Acc: 81.89%\n",
      "Epoch: 33, Train Loss: 1.048\n",
      "Test Loss: 0.538, Test Acc: 82.08%\n",
      "Epoch: 34, Train Loss: 1.114\n",
      "Test Loss: 0.980, Test Acc: 68.64%\n",
      "Epoch: 35, Train Loss: 1.080\n",
      "Test Loss: 0.748, Test Acc: 74.90%\n",
      "Epoch: 36, Train Loss: 1.085\n",
      "Test Loss: 0.791, Test Acc: 75.36%\n",
      "Epoch: 37, Train Loss: 1.096\n",
      "Test Loss: 0.624, Test Acc: 81.68%\n",
      "Epoch: 38, Train Loss: 1.071\n",
      "Test Loss: 0.602, Test Acc: 80.33%\n",
      "Epoch: 39, Train Loss: 1.053\n",
      "Test Loss: 0.551, Test Acc: 83.40%\n",
      "Epoch: 40, Train Loss: 1.044\n",
      "Test Loss: 0.756, Test Acc: 75.79%\n",
      "Epoch: 41, Train Loss: 1.067\n",
      "Test Loss: 0.545, Test Acc: 82.77%\n",
      "Epoch: 42, Train Loss: 1.040\n",
      "Test Loss: 0.574, Test Acc: 82.18%\n",
      "Epoch: 43, Train Loss: 1.041\n",
      "Test Loss: 0.624, Test Acc: 80.39%\n",
      "Epoch: 44, Train Loss: 1.067\n",
      "Test Loss: 0.622, Test Acc: 80.26%\n",
      "Epoch: 45, Train Loss: 1.033\n",
      "Test Loss: 0.534, Test Acc: 83.13%\n",
      "Epoch: 46, Train Loss: 1.060\n",
      "Test Loss: 0.736, Test Acc: 76.25%\n",
      "Epoch: 47, Train Loss: 1.021\n",
      "Test Loss: 0.522, Test Acc: 83.60%\n",
      "Epoch: 48, Train Loss: 1.032\n",
      "Test Loss: 0.670, Test Acc: 77.63%\n",
      "Epoch: 49, Train Loss: 1.053\n",
      "Test Loss: 0.528, Test Acc: 82.59%\n",
      "Epoch: 50, Train Loss: 1.045\n",
      "Test Loss: 0.593, Test Acc: 80.61%\n",
      "Epoch: 51, Train Loss: 1.054\n",
      "Test Loss: 0.583, Test Acc: 81.01%\n",
      "Epoch: 52, Train Loss: 1.007\n",
      "Test Loss: 0.699, Test Acc: 77.11%\n",
      "Epoch: 53, Train Loss: 1.056\n",
      "Test Loss: 0.636, Test Acc: 79.82%\n",
      "Epoch: 54, Train Loss: 1.088\n",
      "Test Loss: 0.606, Test Acc: 81.17%\n",
      "Epoch: 55, Train Loss: 1.042\n",
      "Test Loss: 0.551, Test Acc: 82.70%\n",
      "Epoch: 56, Train Loss: 1.027\n",
      "Test Loss: 0.671, Test Acc: 77.99%\n",
      "Epoch: 57, Train Loss: 0.992\n",
      "Test Loss: 0.542, Test Acc: 82.45%\n",
      "Epoch: 58, Train Loss: 1.038\n",
      "Test Loss: 0.627, Test Acc: 81.11%\n",
      "Epoch: 59, Train Loss: 1.003\n",
      "Test Loss: 0.596, Test Acc: 81.40%\n",
      "Epoch: 60, Train Loss: 1.024\n",
      "Test Loss: 0.636, Test Acc: 81.19%\n",
      "Epoch: 61, Train Loss: 0.999\n",
      "Test Loss: 0.617, Test Acc: 81.32%\n",
      "Epoch: 62, Train Loss: 0.992\n",
      "Test Loss: 0.558, Test Acc: 82.71%\n",
      "Epoch: 63, Train Loss: 1.034\n",
      "Test Loss: 0.573, Test Acc: 82.06%\n",
      "Epoch: 64, Train Loss: 1.039\n",
      "Test Loss: 0.568, Test Acc: 81.97%\n",
      "Epoch: 65, Train Loss: 1.013\n",
      "Test Loss: 0.533, Test Acc: 84.17%\n",
      "Epoch: 66, Train Loss: 1.013\n",
      "Test Loss: 0.568, Test Acc: 82.87%\n",
      "Epoch: 67, Train Loss: 0.962\n",
      "Test Loss: 0.512, Test Acc: 83.93%\n",
      "Epoch: 68, Train Loss: 0.970\n",
      "Test Loss: 0.504, Test Acc: 84.29%\n",
      "Epoch: 69, Train Loss: 0.985\n",
      "Test Loss: 0.535, Test Acc: 83.53%\n",
      "Epoch: 70, Train Loss: 0.985\n",
      "Test Loss: 0.560, Test Acc: 83.72%\n",
      "Epoch: 71, Train Loss: 1.012\n",
      "Test Loss: 0.537, Test Acc: 83.82%\n",
      "Epoch: 72, Train Loss: 0.979\n",
      "Test Loss: 0.483, Test Acc: 84.57%\n",
      "Epoch: 73, Train Loss: 0.980\n",
      "Test Loss: 0.483, Test Acc: 84.42%\n",
      "Epoch: 74, Train Loss: 0.978\n",
      "Test Loss: 0.489, Test Acc: 84.31%\n",
      "Epoch: 75, Train Loss: 0.955\n",
      "Test Loss: 0.461, Test Acc: 85.71%\n",
      "Epoch: 76, Train Loss: 0.956\n",
      "Test Loss: 0.522, Test Acc: 83.16%\n",
      "Epoch: 77, Train Loss: 0.946\n",
      "Test Loss: 0.483, Test Acc: 85.19%\n",
      "Epoch: 78, Train Loss: 0.943\n",
      "Test Loss: 0.481, Test Acc: 85.81%\n",
      "Epoch: 79, Train Loss: 0.961\n",
      "Test Loss: 0.488, Test Acc: 86.72%\n",
      "Epoch: 80, Train Loss: 0.981\n",
      "Test Loss: 0.497, Test Acc: 85.18%\n",
      "Epoch: 81, Train Loss: 0.963\n",
      "Test Loss: 0.504, Test Acc: 85.09%\n",
      "Epoch: 82, Train Loss: 0.940\n",
      "Test Loss: 0.499, Test Acc: 86.67%\n",
      "Epoch: 83, Train Loss: 0.938\n",
      "Test Loss: 0.456, Test Acc: 87.74%\n",
      "Epoch: 84, Train Loss: 0.915\n",
      "Test Loss: 0.462, Test Acc: 86.44%\n",
      "Epoch: 85, Train Loss: 0.918\n",
      "Test Loss: 0.544, Test Acc: 82.81%\n",
      "Epoch: 86, Train Loss: 0.937\n",
      "Test Loss: 0.450, Test Acc: 86.46%\n",
      "Epoch: 87, Train Loss: 0.959\n",
      "Test Loss: 0.409, Test Acc: 87.78%\n",
      "Epoch: 88, Train Loss: 0.904\n",
      "Test Loss: 0.377, Test Acc: 87.73%\n",
      "Epoch: 89, Train Loss: 0.897\n",
      "Test Loss: 0.475, Test Acc: 85.13%\n",
      "Epoch: 90, Train Loss: 0.934\n",
      "Test Loss: 0.416, Test Acc: 88.35%\n",
      "Epoch: 91, Train Loss: 0.906\n",
      "Test Loss: 0.485, Test Acc: 84.53%\n",
      "Epoch: 92, Train Loss: 0.888\n",
      "Test Loss: 0.373, Test Acc: 88.48%\n",
      "Epoch: 93, Train Loss: 0.910\n",
      "Test Loss: 0.432, Test Acc: 87.58%\n",
      "Epoch: 94, Train Loss: 0.874\n",
      "Test Loss: 0.379, Test Acc: 88.17%\n",
      "Epoch: 95, Train Loss: 0.905\n",
      "Test Loss: 0.436, Test Acc: 87.07%\n",
      "Epoch: 96, Train Loss: 0.896\n",
      "Test Loss: 0.376, Test Acc: 88.65%\n",
      "Epoch: 97, Train Loss: 0.886\n",
      "Test Loss: 0.399, Test Acc: 88.41%\n",
      "Epoch: 98, Train Loss: 0.846\n",
      "Test Loss: 0.464, Test Acc: 85.47%\n",
      "Epoch: 99, Train Loss: 0.859\n",
      "Test Loss: 0.384, Test Acc: 88.47%\n",
      "Epoch: 100, Train Loss: 0.908\n",
      "Test Loss: 0.357, Test Acc: 89.49%\n",
      "Epoch: 101, Train Loss: 0.864\n",
      "Test Loss: 0.399, Test Acc: 88.94%\n",
      "Epoch: 102, Train Loss: 0.877\n",
      "Test Loss: 0.378, Test Acc: 88.92%\n",
      "Epoch: 103, Train Loss: 0.847\n",
      "Test Loss: 0.381, Test Acc: 89.79%\n",
      "Epoch: 104, Train Loss: 0.788\n",
      "Test Loss: 0.415, Test Acc: 88.37%\n",
      "Epoch: 105, Train Loss: 0.815\n",
      "Test Loss: 0.363, Test Acc: 90.26%\n",
      "Epoch: 106, Train Loss: 0.820\n",
      "Test Loss: 0.362, Test Acc: 89.10%\n",
      "Epoch: 107, Train Loss: 0.819\n",
      "Test Loss: 0.380, Test Acc: 88.51%\n",
      "Epoch: 108, Train Loss: 0.814\n",
      "Test Loss: 0.325, Test Acc: 90.11%\n",
      "Epoch: 109, Train Loss: 0.836\n",
      "Test Loss: 0.334, Test Acc: 90.75%\n",
      "Epoch: 110, Train Loss: 0.827\n",
      "Test Loss: 0.328, Test Acc: 90.92%\n",
      "Epoch: 111, Train Loss: 0.835\n",
      "Test Loss: 0.382, Test Acc: 91.22%\n",
      "Epoch: 112, Train Loss: 0.834\n",
      "Test Loss: 0.324, Test Acc: 91.62%\n",
      "Epoch: 113, Train Loss: 0.830\n",
      "Test Loss: 0.320, Test Acc: 90.77%\n",
      "Epoch: 114, Train Loss: 0.773\n",
      "Test Loss: 0.307, Test Acc: 91.92%\n",
      "Epoch: 115, Train Loss: 0.792\n",
      "Test Loss: 0.309, Test Acc: 91.33%\n",
      "Epoch: 116, Train Loss: 0.783\n",
      "Test Loss: 0.350, Test Acc: 91.22%\n",
      "Epoch: 117, Train Loss: 0.801\n",
      "Test Loss: 0.327, Test Acc: 91.58%\n",
      "Epoch: 118, Train Loss: 0.747\n",
      "Test Loss: 0.346, Test Acc: 90.95%\n",
      "Epoch: 119, Train Loss: 0.786\n",
      "Test Loss: 0.271, Test Acc: 92.29%\n",
      "Epoch: 120, Train Loss: 0.750\n",
      "Test Loss: 0.267, Test Acc: 92.03%\n",
      "Epoch: 121, Train Loss: 0.774\n",
      "Test Loss: 0.302, Test Acc: 92.45%\n",
      "Epoch: 122, Train Loss: 0.802\n",
      "Test Loss: 0.270, Test Acc: 92.43%\n",
      "Epoch: 123, Train Loss: 0.750\n",
      "Test Loss: 0.284, Test Acc: 92.56%\n",
      "Epoch: 124, Train Loss: 0.722\n",
      "Test Loss: 0.268, Test Acc: 92.51%\n",
      "Epoch: 125, Train Loss: 0.752\n",
      "Test Loss: 0.267, Test Acc: 92.96%\n",
      "Epoch: 126, Train Loss: 0.691\n",
      "Test Loss: 0.277, Test Acc: 93.33%\n",
      "Epoch: 127, Train Loss: 0.691\n",
      "Test Loss: 0.255, Test Acc: 93.04%\n",
      "Epoch: 128, Train Loss: 0.696\n",
      "Test Loss: 0.294, Test Acc: 93.36%\n",
      "Epoch: 129, Train Loss: 0.694\n",
      "Test Loss: 0.236, Test Acc: 93.41%\n",
      "Epoch: 130, Train Loss: 0.683\n",
      "Test Loss: 0.223, Test Acc: 93.64%\n",
      "Epoch: 131, Train Loss: 0.638\n",
      "Test Loss: 0.258, Test Acc: 93.60%\n",
      "Epoch: 132, Train Loss: 0.717\n",
      "Test Loss: 0.279, Test Acc: 93.73%\n",
      "Epoch: 133, Train Loss: 0.662\n",
      "Test Loss: 0.281, Test Acc: 93.84%\n",
      "Epoch: 134, Train Loss: 0.658\n",
      "Test Loss: 0.263, Test Acc: 93.90%\n",
      "Epoch: 135, Train Loss: 0.667\n",
      "Test Loss: 0.258, Test Acc: 94.04%\n",
      "Epoch: 136, Train Loss: 0.670\n",
      "Test Loss: 0.279, Test Acc: 93.72%\n",
      "Epoch: 137, Train Loss: 0.645\n",
      "Test Loss: 0.253, Test Acc: 94.36%\n",
      "Epoch: 138, Train Loss: 0.650\n",
      "Test Loss: 0.240, Test Acc: 94.17%\n",
      "Epoch: 139, Train Loss: 0.649\n",
      "Test Loss: 0.252, Test Acc: 94.28%\n",
      "Epoch: 140, Train Loss: 0.632\n",
      "Test Loss: 0.241, Test Acc: 94.47%\n",
      "Epoch: 141, Train Loss: 0.639\n",
      "Test Loss: 0.211, Test Acc: 94.59%\n",
      "Epoch: 142, Train Loss: 0.684\n",
      "Test Loss: 0.232, Test Acc: 94.53%\n",
      "Epoch: 143, Train Loss: 0.638\n",
      "Test Loss: 0.204, Test Acc: 94.67%\n",
      "Epoch: 144, Train Loss: 0.606\n",
      "Test Loss: 0.239, Test Acc: 94.59%\n",
      "Epoch: 145, Train Loss: 0.627\n",
      "Test Loss: 0.252, Test Acc: 94.66%\n",
      "Epoch: 146, Train Loss: 0.646\n",
      "Test Loss: 0.258, Test Acc: 94.77%\n",
      "Epoch: 147, Train Loss: 0.641\n",
      "Test Loss: 0.209, Test Acc: 94.88%\n",
      "Epoch: 148, Train Loss: 0.656\n",
      "Test Loss: 0.214, Test Acc: 94.79%\n",
      "Epoch: 149, Train Loss: 0.636\n",
      "Test Loss: 0.234, Test Acc: 94.79%\n",
      "Epoch: 150, Train Loss: 0.663\n",
      "Test Loss: 0.244, Test Acc: 94.80%\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇█████</td></tr><tr><td>test_accuracy</td><td>▁▄▆▅▅▄▆▆▆▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>test_loss</td><td>█▅▅▅▇▅▅▄▇▄▆▄▄▅▅▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▅▅▄▅▄▄▄▄▄▄▄▄▄▄▄▃▄▃▃▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_accuracy</td><td>94.88</td></tr><tr><td>epoch</td><td>150</td></tr><tr><td>final_accuracy</td><td>94.8</td></tr><tr><td>test_accuracy</td><td>94.8</td></tr><tr><td>test_loss</td><td>0.2444</td></tr><tr><td>train_loss</td><td>0.66297</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">visionary-sweep-2</strong> at: <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/i8krnwwp' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/i8krnwwp</a><br> View project at: <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250307_123139-i8krnwwp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: accyczi9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcolor_jitter_brightness: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thorizontal_flip: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmixup_alpha: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 150\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_crop: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_erasing_prob: 0.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/JL/Desktop/DL_Proj/wandb/run-20250307_134353-accyczi9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/accyczi9' target=\"_blank\">gallant-sweep-3</a></strong> to <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/accyczi9' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/accyczi9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS for acceleration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JL/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/JL/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2,797,610\n",
      "Trainable parameters: 2,797,610\n",
      "Epoch: 1, Train Loss: 2.146\n",
      "Test Loss: 1.674, Test Acc: 37.95%\n",
      "Epoch: 2, Train Loss: 1.939\n",
      "Test Loss: 1.484, Test Acc: 45.31%\n",
      "Epoch: 3, Train Loss: 1.757\n",
      "Test Loss: 1.408, Test Acc: 50.95%\n",
      "Epoch: 4, Train Loss: 1.618\n",
      "Test Loss: 1.204, Test Acc: 57.97%\n",
      "Epoch: 5, Train Loss: 1.544\n",
      "Test Loss: 1.020, Test Acc: 66.49%\n",
      "Epoch: 6, Train Loss: 1.469\n",
      "Test Loss: 0.972, Test Acc: 68.28%\n",
      "Epoch: 7, Train Loss: 1.410\n",
      "Test Loss: 1.084, Test Acc: 62.23%\n",
      "Epoch: 8, Train Loss: 1.410\n",
      "Test Loss: 0.910, Test Acc: 70.18%\n",
      "Epoch: 9, Train Loss: 1.385\n",
      "Test Loss: 0.886, Test Acc: 70.98%\n",
      "Epoch: 10, Train Loss: 1.382\n",
      "Test Loss: 0.889, Test Acc: 69.65%\n",
      "Epoch: 11, Train Loss: 1.319\n",
      "Test Loss: 1.062, Test Acc: 64.85%\n",
      "Epoch: 12, Train Loss: 1.376\n",
      "Test Loss: 0.749, Test Acc: 75.75%\n",
      "Epoch: 13, Train Loss: 1.321\n",
      "Test Loss: 0.741, Test Acc: 75.67%\n",
      "Epoch: 14, Train Loss: 1.310\n",
      "Test Loss: 0.793, Test Acc: 74.97%\n",
      "Epoch: 15, Train Loss: 1.277\n",
      "Test Loss: 0.931, Test Acc: 68.62%\n",
      "Epoch: 16, Train Loss: 1.298\n",
      "Test Loss: 0.864, Test Acc: 71.71%\n",
      "Epoch: 17, Train Loss: 1.286\n",
      "Test Loss: 0.744, Test Acc: 77.35%\n",
      "Epoch: 18, Train Loss: 1.245\n",
      "Test Loss: 0.743, Test Acc: 76.60%\n",
      "Epoch: 19, Train Loss: 1.261\n",
      "Test Loss: 0.744, Test Acc: 76.41%\n",
      "Epoch: 20, Train Loss: 1.265\n",
      "Test Loss: 0.736, Test Acc: 76.59%\n",
      "Epoch: 21, Train Loss: 1.264\n",
      "Test Loss: 0.718, Test Acc: 76.90%\n",
      "Epoch: 22, Train Loss: 1.251\n",
      "Test Loss: 0.679, Test Acc: 79.86%\n",
      "Epoch: 23, Train Loss: 1.256\n",
      "Test Loss: 0.675, Test Acc: 79.13%\n",
      "Epoch: 24, Train Loss: 1.235\n",
      "Test Loss: 0.736, Test Acc: 76.58%\n",
      "Epoch: 25, Train Loss: 1.203\n",
      "Test Loss: 0.708, Test Acc: 77.02%\n",
      "Epoch: 26, Train Loss: 1.279\n",
      "Test Loss: 0.685, Test Acc: 80.99%\n",
      "Epoch: 27, Train Loss: 1.253\n",
      "Test Loss: 0.772, Test Acc: 75.54%\n",
      "Epoch: 28, Train Loss: 1.250\n",
      "Test Loss: 0.657, Test Acc: 79.05%\n",
      "Epoch: 29, Train Loss: 1.249\n",
      "Test Loss: 0.711, Test Acc: 79.13%\n",
      "Epoch: 30, Train Loss: 1.215\n",
      "Test Loss: 0.734, Test Acc: 77.00%\n",
      "Epoch: 31, Train Loss: 1.243\n",
      "Test Loss: 0.795, Test Acc: 75.19%\n",
      "Epoch: 32, Train Loss: 1.226\n",
      "Test Loss: 0.846, Test Acc: 72.52%\n",
      "Epoch: 33, Train Loss: 1.223\n",
      "Test Loss: 0.664, Test Acc: 80.73%\n",
      "Epoch: 34, Train Loss: 1.232\n",
      "Test Loss: 0.673, Test Acc: 80.54%\n",
      "Epoch: 35, Train Loss: 1.222\n",
      "Test Loss: 0.633, Test Acc: 81.19%\n",
      "Epoch: 36, Train Loss: 1.193\n",
      "Test Loss: 0.633, Test Acc: 79.09%\n",
      "Epoch: 37, Train Loss: 1.239\n",
      "Test Loss: 0.644, Test Acc: 81.89%\n",
      "Epoch: 38, Train Loss: 1.225\n",
      "Test Loss: 0.689, Test Acc: 79.31%\n",
      "Epoch: 39, Train Loss: 1.212\n",
      "Test Loss: 0.616, Test Acc: 80.83%\n",
      "Epoch: 40, Train Loss: 1.223\n",
      "Test Loss: 0.738, Test Acc: 77.17%\n",
      "Epoch: 41, Train Loss: 1.193\n",
      "Test Loss: 0.577, Test Acc: 82.26%\n",
      "Epoch: 42, Train Loss: 1.179\n",
      "Test Loss: 0.513, Test Acc: 83.75%\n",
      "Epoch: 43, Train Loss: 1.188\n",
      "Test Loss: 0.617, Test Acc: 81.85%\n",
      "Epoch: 44, Train Loss: 1.226\n",
      "Test Loss: 0.636, Test Acc: 81.28%\n",
      "Epoch: 45, Train Loss: 1.174\n",
      "Test Loss: 0.644, Test Acc: 78.83%\n",
      "Epoch: 46, Train Loss: 1.194\n",
      "Test Loss: 0.605, Test Acc: 81.68%\n",
      "Epoch: 47, Train Loss: 1.182\n",
      "Test Loss: 0.703, Test Acc: 77.86%\n",
      "Epoch: 48, Train Loss: 1.197\n",
      "Test Loss: 0.552, Test Acc: 82.55%\n",
      "Epoch: 49, Train Loss: 1.169\n",
      "Test Loss: 0.666, Test Acc: 79.61%\n",
      "Epoch: 50, Train Loss: 1.184\n",
      "Test Loss: 0.721, Test Acc: 78.03%\n",
      "Epoch: 51, Train Loss: 1.184\n",
      "Test Loss: 0.549, Test Acc: 84.05%\n",
      "Epoch: 52, Train Loss: 1.188\n",
      "Test Loss: 0.570, Test Acc: 83.80%\n",
      "Epoch: 53, Train Loss: 1.132\n",
      "Test Loss: 0.575, Test Acc: 81.91%\n",
      "Epoch: 54, Train Loss: 1.186\n",
      "Test Loss: 0.699, Test Acc: 79.37%\n",
      "Epoch: 55, Train Loss: 1.157\n",
      "Test Loss: 0.596, Test Acc: 83.38%\n",
      "Epoch: 56, Train Loss: 1.140\n",
      "Test Loss: 0.659, Test Acc: 78.92%\n",
      "Epoch: 57, Train Loss: 1.140\n",
      "Test Loss: 0.581, Test Acc: 81.39%\n",
      "Epoch: 58, Train Loss: 1.171\n",
      "Test Loss: 0.580, Test Acc: 82.83%\n",
      "Epoch: 59, Train Loss: 1.144\n",
      "Test Loss: 0.507, Test Acc: 85.38%\n",
      "Epoch: 60, Train Loss: 1.141\n",
      "Test Loss: 0.579, Test Acc: 82.76%\n",
      "Epoch: 61, Train Loss: 1.148\n",
      "Test Loss: 0.612, Test Acc: 79.59%\n",
      "Epoch: 62, Train Loss: 1.150\n",
      "Test Loss: 0.595, Test Acc: 80.89%\n",
      "Epoch: 63, Train Loss: 1.176\n",
      "Test Loss: 0.502, Test Acc: 85.06%\n",
      "Epoch: 64, Train Loss: 1.141\n",
      "Test Loss: 0.666, Test Acc: 79.13%\n",
      "Epoch: 65, Train Loss: 1.157\n",
      "Test Loss: 0.583, Test Acc: 82.10%\n",
      "Epoch: 66, Train Loss: 1.192\n",
      "Test Loss: 0.579, Test Acc: 84.15%\n",
      "Epoch: 67, Train Loss: 1.134\n",
      "Test Loss: 0.505, Test Acc: 85.59%\n",
      "Epoch: 68, Train Loss: 1.142\n",
      "Test Loss: 0.507, Test Acc: 85.00%\n",
      "Epoch: 69, Train Loss: 1.134\n",
      "Test Loss: 0.555, Test Acc: 85.59%\n",
      "Epoch: 70, Train Loss: 1.137\n",
      "Test Loss: 0.527, Test Acc: 83.96%\n",
      "Epoch: 71, Train Loss: 1.139\n",
      "Test Loss: 0.598, Test Acc: 82.57%\n",
      "Epoch: 72, Train Loss: 1.162\n",
      "Test Loss: 0.546, Test Acc: 86.00%\n",
      "Epoch: 73, Train Loss: 1.105\n",
      "Test Loss: 0.574, Test Acc: 82.55%\n",
      "Epoch: 74, Train Loss: 1.128\n",
      "Test Loss: 0.535, Test Acc: 84.34%\n",
      "Epoch: 75, Train Loss: 1.127\n",
      "Test Loss: 0.438, Test Acc: 86.49%\n",
      "Epoch: 76, Train Loss: 1.153\n",
      "Test Loss: 0.524, Test Acc: 85.47%\n",
      "Epoch: 77, Train Loss: 1.090\n",
      "Test Loss: 0.467, Test Acc: 86.24%\n",
      "Epoch: 78, Train Loss: 1.125\n",
      "Test Loss: 0.440, Test Acc: 86.39%\n",
      "Epoch: 79, Train Loss: 1.101\n",
      "Test Loss: 0.500, Test Acc: 85.28%\n",
      "Epoch: 80, Train Loss: 1.101\n",
      "Test Loss: 0.584, Test Acc: 82.20%\n",
      "Epoch: 81, Train Loss: 1.116\n",
      "Test Loss: 0.565, Test Acc: 86.05%\n",
      "Epoch: 82, Train Loss: 1.081\n",
      "Test Loss: 0.523, Test Acc: 84.64%\n",
      "Epoch: 83, Train Loss: 1.100\n",
      "Test Loss: 0.447, Test Acc: 87.15%\n",
      "Epoch: 84, Train Loss: 1.081\n",
      "Test Loss: 0.482, Test Acc: 85.93%\n",
      "Epoch: 85, Train Loss: 1.065\n",
      "Test Loss: 0.419, Test Acc: 87.31%\n",
      "Epoch: 86, Train Loss: 1.103\n",
      "Test Loss: 0.493, Test Acc: 84.49%\n",
      "Epoch: 87, Train Loss: 1.073\n",
      "Test Loss: 0.469, Test Acc: 87.14%\n",
      "Epoch: 88, Train Loss: 1.061\n",
      "Test Loss: 0.465, Test Acc: 88.43%\n",
      "Epoch: 89, Train Loss: 1.101\n",
      "Test Loss: 0.494, Test Acc: 86.12%\n",
      "Epoch: 90, Train Loss: 1.081\n",
      "Test Loss: 0.434, Test Acc: 87.44%\n",
      "Epoch: 91, Train Loss: 1.058\n",
      "Test Loss: 0.471, Test Acc: 85.66%\n",
      "Epoch: 92, Train Loss: 1.051\n",
      "Test Loss: 0.439, Test Acc: 88.29%\n",
      "Epoch: 93, Train Loss: 1.071\n",
      "Test Loss: 0.522, Test Acc: 86.11%\n",
      "Epoch: 94, Train Loss: 1.054\n",
      "Test Loss: 0.445, Test Acc: 87.72%\n",
      "Epoch: 95, Train Loss: 1.029\n",
      "Test Loss: 0.463, Test Acc: 88.12%\n",
      "Epoch: 96, Train Loss: 1.020\n",
      "Test Loss: 0.384, Test Acc: 88.13%\n",
      "Epoch: 97, Train Loss: 1.015\n",
      "Test Loss: 0.461, Test Acc: 88.47%\n",
      "Epoch: 98, Train Loss: 1.053\n",
      "Test Loss: 0.439, Test Acc: 86.95%\n",
      "Epoch: 99, Train Loss: 1.014\n",
      "Test Loss: 0.397, Test Acc: 89.38%\n",
      "Epoch: 100, Train Loss: 1.013\n",
      "Test Loss: 0.393, Test Acc: 86.92%\n",
      "Epoch: 101, Train Loss: 1.056\n",
      "Test Loss: 0.394, Test Acc: 89.29%\n",
      "Epoch: 102, Train Loss: 1.007\n",
      "Test Loss: 0.354, Test Acc: 89.99%\n",
      "Epoch: 103, Train Loss: 0.972\n",
      "Test Loss: 0.403, Test Acc: 87.51%\n",
      "Epoch: 104, Train Loss: 1.058\n",
      "Test Loss: 0.420, Test Acc: 90.23%\n",
      "Epoch: 105, Train Loss: 1.008\n",
      "Test Loss: 0.432, Test Acc: 88.18%\n",
      "Epoch: 106, Train Loss: 1.036\n",
      "Test Loss: 0.388, Test Acc: 89.40%\n",
      "Epoch: 107, Train Loss: 1.044\n",
      "Test Loss: 0.338, Test Acc: 90.82%\n",
      "Epoch: 108, Train Loss: 0.961\n",
      "Test Loss: 0.368, Test Acc: 88.93%\n",
      "Epoch: 109, Train Loss: 0.990\n",
      "Test Loss: 0.379, Test Acc: 90.28%\n",
      "Epoch: 110, Train Loss: 0.978\n",
      "Test Loss: 0.352, Test Acc: 91.08%\n",
      "Epoch: 111, Train Loss: 0.957\n",
      "Test Loss: 0.340, Test Acc: 90.72%\n",
      "Epoch: 112, Train Loss: 0.955\n",
      "Test Loss: 0.359, Test Acc: 91.01%\n",
      "Epoch: 113, Train Loss: 0.950\n",
      "Test Loss: 0.310, Test Acc: 91.23%\n",
      "Epoch: 114, Train Loss: 0.979\n",
      "Test Loss: 0.392, Test Acc: 90.04%\n",
      "Epoch: 115, Train Loss: 1.000\n",
      "Test Loss: 0.319, Test Acc: 91.12%\n",
      "Epoch: 116, Train Loss: 0.955\n",
      "Test Loss: 0.322, Test Acc: 91.43%\n",
      "Epoch: 117, Train Loss: 0.979\n",
      "Test Loss: 0.307, Test Acc: 91.86%\n",
      "Epoch: 118, Train Loss: 0.928\n",
      "Test Loss: 0.331, Test Acc: 91.79%\n",
      "Epoch: 119, Train Loss: 0.961\n",
      "Test Loss: 0.290, Test Acc: 92.11%\n",
      "Epoch: 120, Train Loss: 0.912\n",
      "Test Loss: 0.283, Test Acc: 92.40%\n",
      "Epoch: 121, Train Loss: 0.905\n",
      "Test Loss: 0.322, Test Acc: 92.20%\n",
      "Epoch: 122, Train Loss: 0.931\n",
      "Test Loss: 0.279, Test Acc: 92.43%\n",
      "Epoch: 123, Train Loss: 0.930\n",
      "Test Loss: 0.303, Test Acc: 92.61%\n",
      "Epoch: 124, Train Loss: 0.884\n",
      "Test Loss: 0.340, Test Acc: 91.94%\n",
      "Epoch: 125, Train Loss: 0.890\n",
      "Test Loss: 0.289, Test Acc: 92.87%\n",
      "Epoch: 126, Train Loss: 0.907\n",
      "Test Loss: 0.280, Test Acc: 92.78%\n",
      "Epoch: 127, Train Loss: 0.938\n",
      "Test Loss: 0.266, Test Acc: 92.92%\n",
      "Epoch: 128, Train Loss: 0.889\n",
      "Test Loss: 0.303, Test Acc: 93.20%\n",
      "Epoch: 129, Train Loss: 0.888\n",
      "Test Loss: 0.269, Test Acc: 93.42%\n",
      "Epoch: 130, Train Loss: 0.876\n",
      "Test Loss: 0.266, Test Acc: 93.63%\n",
      "Epoch: 131, Train Loss: 0.856\n",
      "Test Loss: 0.286, Test Acc: 93.19%\n",
      "Epoch: 132, Train Loss: 0.846\n",
      "Test Loss: 0.259, Test Acc: 93.51%\n",
      "Epoch: 133, Train Loss: 0.891\n",
      "Test Loss: 0.237, Test Acc: 93.75%\n",
      "Epoch: 134, Train Loss: 0.889\n",
      "Test Loss: 0.249, Test Acc: 93.66%\n",
      "Epoch: 135, Train Loss: 0.890\n",
      "Test Loss: 0.242, Test Acc: 94.08%\n",
      "Epoch: 136, Train Loss: 0.860\n",
      "Test Loss: 0.271, Test Acc: 94.09%\n",
      "Epoch: 137, Train Loss: 0.832\n",
      "Test Loss: 0.292, Test Acc: 94.10%\n",
      "Epoch: 138, Train Loss: 0.835\n",
      "Test Loss: 0.316, Test Acc: 94.24%\n",
      "Epoch: 139, Train Loss: 0.833\n",
      "Test Loss: 0.239, Test Acc: 94.51%\n",
      "Epoch: 140, Train Loss: 0.870\n",
      "Test Loss: 0.217, Test Acc: 94.47%\n",
      "Epoch: 141, Train Loss: 0.808\n",
      "Test Loss: 0.267, Test Acc: 94.42%\n",
      "Epoch: 142, Train Loss: 0.823\n",
      "Test Loss: 0.215, Test Acc: 94.75%\n",
      "Epoch: 143, Train Loss: 0.821\n",
      "Test Loss: 0.256, Test Acc: 94.58%\n",
      "Epoch: 144, Train Loss: 0.789\n",
      "Test Loss: 0.228, Test Acc: 94.65%\n",
      "Epoch: 145, Train Loss: 0.793\n",
      "Test Loss: 0.198, Test Acc: 94.77%\n",
      "Epoch: 146, Train Loss: 0.844\n",
      "Test Loss: 0.246, Test Acc: 94.68%\n",
      "Epoch: 147, Train Loss: 0.809\n",
      "Test Loss: 0.219, Test Acc: 94.68%\n",
      "Epoch: 148, Train Loss: 0.835\n",
      "Test Loss: 0.226, Test Acc: 94.75%\n",
      "Epoch: 149, Train Loss: 0.793\n",
      "Test Loss: 0.217, Test Acc: 94.83%\n",
      "Epoch: 150, Train Loss: 0.809\n",
      "Test Loss: 0.227, Test Acc: 94.70%\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇█</td></tr><tr><td>test_accuracy</td><td>▁▃▃▅▄▅▆▆▆▆▆▆▆▇▆▆▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇████████</td></tr><tr><td>test_loss</td><td>█▇▇▆▅▄▃▃▃▃▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_accuracy</td><td>94.83</td></tr><tr><td>epoch</td><td>150</td></tr><tr><td>final_accuracy</td><td>94.7</td></tr><tr><td>test_accuracy</td><td>94.7</td></tr><tr><td>test_loss</td><td>0.22685</td></tr><tr><td>train_loss</td><td>0.80899</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gallant-sweep-3</strong> at: <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/accyczi9' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/accyczi9</a><br> View project at: <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250307_134353-accyczi9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: exoic84s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcolor_jitter_brightness: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thorizontal_flip: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmixup_alpha: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 150\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_crop: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_erasing_prob: 0.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/JL/Desktop/DL_Proj/wandb/run-20250307_145507-exoic84s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/exoic84s' target=\"_blank\">sweet-sweep-4</a></strong> to <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/exoic84s' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/exoic84s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS for acceleration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JL/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/JL/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2,797,610\n",
      "Trainable parameters: 2,797,610\n",
      "Epoch: 1, Train Loss: 2.171\n",
      "Test Loss: 1.725, Test Acc: 35.50%\n",
      "Epoch: 2, Train Loss: 1.948\n",
      "Test Loss: 1.867, Test Acc: 31.38%\n",
      "Epoch: 3, Train Loss: 1.807\n",
      "Test Loss: 1.673, Test Acc: 38.37%\n",
      "Epoch: 4, Train Loss: 1.694\n",
      "Test Loss: 1.639, Test Acc: 39.28%\n",
      "Epoch: 5, Train Loss: 1.630\n",
      "Test Loss: 1.464, Test Acc: 49.88%\n",
      "Epoch: 6, Train Loss: 1.558\n",
      "Test Loss: 1.310, Test Acc: 55.87%\n",
      "Epoch: 7, Train Loss: 1.494\n",
      "Test Loss: 0.960, Test Acc: 67.81%\n",
      "Epoch: 8, Train Loss: 1.489\n",
      "Test Loss: 0.976, Test Acc: 66.42%\n",
      "Epoch: 9, Train Loss: 1.429\n",
      "Test Loss: 1.314, Test Acc: 57.32%\n",
      "Epoch: 10, Train Loss: 1.408\n",
      "Test Loss: 0.960, Test Acc: 68.10%\n",
      "Epoch: 11, Train Loss: 1.392\n",
      "Test Loss: 0.804, Test Acc: 74.36%\n",
      "Epoch: 12, Train Loss: 1.373\n",
      "Test Loss: 0.758, Test Acc: 75.55%\n",
      "Epoch: 13, Train Loss: 1.366\n",
      "Test Loss: 0.859, Test Acc: 72.12%\n",
      "Epoch: 14, Train Loss: 1.362\n",
      "Test Loss: 0.935, Test Acc: 69.43%\n",
      "Epoch: 15, Train Loss: 1.342\n",
      "Test Loss: 0.914, Test Acc: 70.81%\n",
      "Epoch: 16, Train Loss: 1.314\n",
      "Test Loss: 0.729, Test Acc: 77.16%\n",
      "Epoch: 17, Train Loss: 1.294\n",
      "Test Loss: 0.810, Test Acc: 74.42%\n",
      "Epoch: 18, Train Loss: 1.342\n",
      "Test Loss: 0.684, Test Acc: 78.57%\n",
      "Epoch: 19, Train Loss: 1.272\n",
      "Test Loss: 0.724, Test Acc: 76.76%\n",
      "Epoch: 20, Train Loss: 1.311\n",
      "Test Loss: 0.815, Test Acc: 73.16%\n",
      "Epoch: 21, Train Loss: 1.329\n",
      "Test Loss: 0.731, Test Acc: 78.75%\n",
      "Epoch: 22, Train Loss: 1.292\n",
      "Test Loss: 0.783, Test Acc: 75.80%\n",
      "Epoch: 23, Train Loss: 1.297\n",
      "Test Loss: 0.807, Test Acc: 72.55%\n",
      "Epoch: 24, Train Loss: 1.337\n",
      "Test Loss: 0.718, Test Acc: 78.40%\n",
      "Epoch: 25, Train Loss: 1.298\n",
      "Test Loss: 0.721, Test Acc: 76.70%\n",
      "Epoch: 26, Train Loss: 1.296\n",
      "Test Loss: 0.894, Test Acc: 70.39%\n",
      "Epoch: 27, Train Loss: 1.279\n",
      "Test Loss: 0.737, Test Acc: 76.66%\n",
      "Epoch: 28, Train Loss: 1.278\n",
      "Test Loss: 0.633, Test Acc: 80.59%\n",
      "Epoch: 29, Train Loss: 1.256\n",
      "Test Loss: 0.667, Test Acc: 78.49%\n",
      "Epoch: 30, Train Loss: 1.283\n",
      "Test Loss: 0.675, Test Acc: 79.08%\n",
      "Epoch: 31, Train Loss: 1.281\n",
      "Test Loss: 0.698, Test Acc: 79.12%\n",
      "Epoch: 32, Train Loss: 1.261\n",
      "Test Loss: 0.719, Test Acc: 78.76%\n",
      "Epoch: 33, Train Loss: 1.263\n",
      "Test Loss: 0.834, Test Acc: 75.69%\n",
      "Epoch: 34, Train Loss: 1.265\n",
      "Test Loss: 0.608, Test Acc: 82.68%\n",
      "Epoch: 35, Train Loss: 1.282\n",
      "Test Loss: 0.767, Test Acc: 77.08%\n",
      "Epoch: 36, Train Loss: 1.246\n",
      "Test Loss: 0.705, Test Acc: 78.08%\n",
      "Epoch: 37, Train Loss: 1.248\n",
      "Test Loss: 0.623, Test Acc: 81.51%\n",
      "Epoch: 38, Train Loss: 1.258\n",
      "Test Loss: 0.719, Test Acc: 77.16%\n",
      "Epoch: 39, Train Loss: 1.257\n",
      "Test Loss: 0.658, Test Acc: 78.08%\n",
      "Epoch: 40, Train Loss: 1.239\n",
      "Test Loss: 0.754, Test Acc: 76.65%\n",
      "Epoch: 41, Train Loss: 1.245\n",
      "Test Loss: 0.806, Test Acc: 74.16%\n",
      "Epoch: 42, Train Loss: 1.205\n",
      "Test Loss: 0.629, Test Acc: 79.97%\n",
      "Epoch: 43, Train Loss: 1.241\n",
      "Test Loss: 0.756, Test Acc: 75.28%\n",
      "Epoch: 44, Train Loss: 1.225\n",
      "Test Loss: 0.657, Test Acc: 80.93%\n",
      "Epoch: 45, Train Loss: 1.252\n",
      "Test Loss: 0.606, Test Acc: 82.04%\n",
      "Epoch: 46, Train Loss: 1.211\n",
      "Test Loss: 0.645, Test Acc: 80.17%\n",
      "Epoch: 47, Train Loss: 1.204\n",
      "Test Loss: 0.667, Test Acc: 77.98%\n",
      "Epoch: 48, Train Loss: 1.235\n",
      "Test Loss: 0.560, Test Acc: 83.24%\n",
      "Epoch: 49, Train Loss: 1.230\n",
      "Test Loss: 0.621, Test Acc: 82.07%\n",
      "Epoch: 50, Train Loss: 1.245\n",
      "Test Loss: 0.594, Test Acc: 82.82%\n",
      "Epoch: 51, Train Loss: 1.210\n",
      "Test Loss: 0.641, Test Acc: 81.66%\n",
      "Epoch: 52, Train Loss: 1.202\n",
      "Test Loss: 0.689, Test Acc: 77.54%\n",
      "Epoch: 53, Train Loss: 1.215\n",
      "Test Loss: 0.733, Test Acc: 78.58%\n",
      "Epoch: 54, Train Loss: 1.187\n",
      "Test Loss: 0.603, Test Acc: 81.20%\n",
      "Epoch: 55, Train Loss: 1.220\n",
      "Test Loss: 0.644, Test Acc: 81.88%\n",
      "Epoch: 56, Train Loss: 1.250\n",
      "Test Loss: 0.661, Test Acc: 80.58%\n",
      "Epoch: 57, Train Loss: 1.216\n",
      "Test Loss: 0.744, Test Acc: 77.00%\n",
      "Epoch: 58, Train Loss: 1.217\n",
      "Test Loss: 0.592, Test Acc: 83.82%\n",
      "Epoch: 59, Train Loss: 1.164\n",
      "Test Loss: 0.683, Test Acc: 77.92%\n",
      "Epoch: 60, Train Loss: 1.190\n",
      "Test Loss: 0.646, Test Acc: 80.03%\n",
      "Epoch: 61, Train Loss: 1.217\n",
      "Test Loss: 0.577, Test Acc: 82.22%\n",
      "Epoch: 62, Train Loss: 1.214\n",
      "Test Loss: 0.584, Test Acc: 83.70%\n",
      "Epoch: 63, Train Loss: 1.232\n",
      "Test Loss: 0.569, Test Acc: 83.98%\n",
      "Epoch: 64, Train Loss: 1.215\n",
      "Test Loss: 0.619, Test Acc: 84.18%\n",
      "Epoch: 65, Train Loss: 1.197\n",
      "Test Loss: 0.636, Test Acc: 82.58%\n",
      "Epoch: 66, Train Loss: 1.185\n",
      "Test Loss: 0.639, Test Acc: 81.20%\n",
      "Epoch: 67, Train Loss: 1.179\n",
      "Test Loss: 0.550, Test Acc: 83.26%\n",
      "Epoch: 68, Train Loss: 1.180\n",
      "Test Loss: 0.561, Test Acc: 85.46%\n",
      "Epoch: 69, Train Loss: 1.185\n",
      "Test Loss: 0.579, Test Acc: 83.76%\n",
      "Epoch: 70, Train Loss: 1.171\n",
      "Test Loss: 0.606, Test Acc: 81.54%\n",
      "Epoch: 71, Train Loss: 1.183\n",
      "Test Loss: 0.592, Test Acc: 82.75%\n",
      "Epoch: 72, Train Loss: 1.133\n",
      "Test Loss: 0.560, Test Acc: 82.66%\n",
      "Epoch: 73, Train Loss: 1.161\n",
      "Test Loss: 0.662, Test Acc: 81.05%\n",
      "Epoch: 74, Train Loss: 1.163\n",
      "Test Loss: 0.559, Test Acc: 84.20%\n",
      "Epoch: 75, Train Loss: 1.141\n",
      "Test Loss: 0.580, Test Acc: 82.01%\n",
      "Epoch: 76, Train Loss: 1.150\n",
      "Test Loss: 0.583, Test Acc: 84.55%\n",
      "Epoch: 77, Train Loss: 1.119\n",
      "Test Loss: 0.550, Test Acc: 84.84%\n",
      "Epoch: 78, Train Loss: 1.160\n",
      "Test Loss: 0.558, Test Acc: 85.69%\n",
      "Epoch: 79, Train Loss: 1.149\n",
      "Test Loss: 0.601, Test Acc: 84.12%\n",
      "Epoch: 80, Train Loss: 1.155\n",
      "Test Loss: 0.532, Test Acc: 86.07%\n",
      "Epoch: 81, Train Loss: 1.110\n",
      "Test Loss: 0.522, Test Acc: 84.19%\n",
      "Epoch: 82, Train Loss: 1.121\n",
      "Test Loss: 0.548, Test Acc: 83.60%\n",
      "Epoch: 83, Train Loss: 1.116\n",
      "Test Loss: 0.625, Test Acc: 82.96%\n",
      "Epoch: 84, Train Loss: 1.129\n",
      "Test Loss: 0.497, Test Acc: 86.39%\n",
      "Epoch: 85, Train Loss: 1.086\n",
      "Test Loss: 0.463, Test Acc: 87.68%\n",
      "Epoch: 86, Train Loss: 1.116\n",
      "Test Loss: 0.503, Test Acc: 85.48%\n",
      "Epoch: 87, Train Loss: 1.110\n",
      "Test Loss: 0.492, Test Acc: 86.64%\n",
      "Epoch: 88, Train Loss: 1.091\n",
      "Test Loss: 0.481, Test Acc: 87.21%\n",
      "Epoch: 89, Train Loss: 1.120\n",
      "Test Loss: 0.457, Test Acc: 87.20%\n",
      "Epoch: 90, Train Loss: 1.089\n",
      "Test Loss: 0.487, Test Acc: 86.71%\n",
      "Epoch: 91, Train Loss: 1.085\n",
      "Test Loss: 0.527, Test Acc: 87.69%\n",
      "Epoch: 92, Train Loss: 1.072\n",
      "Test Loss: 0.438, Test Acc: 87.94%\n",
      "Epoch: 93, Train Loss: 1.065\n",
      "Test Loss: 0.482, Test Acc: 86.26%\n",
      "Epoch: 94, Train Loss: 1.098\n",
      "Test Loss: 0.465, Test Acc: 87.20%\n",
      "Epoch: 95, Train Loss: 1.039\n",
      "Test Loss: 0.492, Test Acc: 87.11%\n",
      "Epoch: 96, Train Loss: 1.088\n",
      "Test Loss: 0.501, Test Acc: 88.20%\n",
      "Epoch: 97, Train Loss: 1.042\n",
      "Test Loss: 0.500, Test Acc: 87.90%\n",
      "Epoch: 98, Train Loss: 1.058\n",
      "Test Loss: 0.414, Test Acc: 87.98%\n",
      "Epoch: 99, Train Loss: 1.072\n",
      "Test Loss: 0.405, Test Acc: 88.87%\n",
      "Epoch: 100, Train Loss: 1.062\n",
      "Test Loss: 0.478, Test Acc: 88.36%\n",
      "Epoch: 101, Train Loss: 1.066\n",
      "Test Loss: 0.428, Test Acc: 89.16%\n",
      "Epoch: 102, Train Loss: 1.063\n",
      "Test Loss: 0.511, Test Acc: 88.13%\n",
      "Epoch: 103, Train Loss: 1.048\n",
      "Test Loss: 0.414, Test Acc: 88.54%\n",
      "Epoch: 104, Train Loss: 1.029\n",
      "Test Loss: 0.443, Test Acc: 88.39%\n",
      "Epoch: 105, Train Loss: 1.022\n",
      "Test Loss: 0.402, Test Acc: 88.86%\n",
      "Epoch: 106, Train Loss: 1.055\n",
      "Test Loss: 0.436, Test Acc: 90.03%\n",
      "Epoch: 107, Train Loss: 1.038\n",
      "Test Loss: 0.446, Test Acc: 89.32%\n",
      "Epoch: 108, Train Loss: 1.030\n",
      "Test Loss: 0.400, Test Acc: 88.71%\n",
      "Epoch: 109, Train Loss: 0.996\n",
      "Test Loss: 0.479, Test Acc: 89.58%\n",
      "Epoch: 110, Train Loss: 1.021\n",
      "Test Loss: 0.432, Test Acc: 89.96%\n",
      "Epoch: 111, Train Loss: 1.015\n",
      "Test Loss: 0.381, Test Acc: 90.28%\n",
      "Epoch: 112, Train Loss: 1.044\n",
      "Test Loss: 0.387, Test Acc: 91.08%\n",
      "Epoch: 113, Train Loss: 1.022\n",
      "Test Loss: 0.410, Test Acc: 91.24%\n",
      "Epoch: 114, Train Loss: 1.014\n",
      "Test Loss: 0.381, Test Acc: 90.63%\n",
      "Epoch: 115, Train Loss: 0.963\n",
      "Test Loss: 0.370, Test Acc: 91.47%\n",
      "Epoch: 116, Train Loss: 0.967\n",
      "Test Loss: 0.353, Test Acc: 90.46%\n",
      "Epoch: 117, Train Loss: 1.017\n",
      "Test Loss: 0.390, Test Acc: 90.99%\n",
      "Epoch: 118, Train Loss: 1.006\n",
      "Test Loss: 0.413, Test Acc: 91.08%\n",
      "Epoch: 119, Train Loss: 0.978\n",
      "Test Loss: 0.349, Test Acc: 92.05%\n",
      "Epoch: 120, Train Loss: 0.959\n",
      "Test Loss: 0.315, Test Acc: 92.41%\n",
      "Epoch: 121, Train Loss: 0.975\n",
      "Test Loss: 0.343, Test Acc: 92.35%\n",
      "Epoch: 122, Train Loss: 0.970\n",
      "Test Loss: 0.324, Test Acc: 91.69%\n",
      "Epoch: 123, Train Loss: 0.970\n",
      "Test Loss: 0.457, Test Acc: 90.42%\n",
      "Epoch: 124, Train Loss: 0.952\n",
      "Test Loss: 0.399, Test Acc: 91.93%\n",
      "Epoch: 125, Train Loss: 0.931\n",
      "Test Loss: 0.352, Test Acc: 92.19%\n",
      "Epoch: 126, Train Loss: 0.897\n",
      "Test Loss: 0.320, Test Acc: 92.21%\n",
      "Epoch: 127, Train Loss: 0.883\n",
      "Test Loss: 0.327, Test Acc: 93.01%\n",
      "Epoch: 128, Train Loss: 0.928\n",
      "Test Loss: 0.321, Test Acc: 93.01%\n",
      "Epoch: 129, Train Loss: 0.928\n",
      "Test Loss: 0.313, Test Acc: 93.44%\n",
      "Epoch: 130, Train Loss: 0.872\n",
      "Test Loss: 0.270, Test Acc: 93.05%\n",
      "Epoch: 131, Train Loss: 0.891\n",
      "Test Loss: 0.294, Test Acc: 93.75%\n",
      "Epoch: 132, Train Loss: 0.909\n",
      "Test Loss: 0.291, Test Acc: 93.93%\n",
      "Epoch: 133, Train Loss: 0.848\n",
      "Test Loss: 0.245, Test Acc: 93.83%\n",
      "Epoch: 134, Train Loss: 0.877\n",
      "Test Loss: 0.250, Test Acc: 93.83%\n",
      "Epoch: 135, Train Loss: 0.904\n",
      "Test Loss: 0.328, Test Acc: 94.03%\n",
      "Epoch: 136, Train Loss: 0.869\n",
      "Test Loss: 0.250, Test Acc: 94.24%\n",
      "Epoch: 137, Train Loss: 0.889\n",
      "Test Loss: 0.270, Test Acc: 94.06%\n",
      "Epoch: 138, Train Loss: 0.884\n",
      "Test Loss: 0.248, Test Acc: 94.34%\n",
      "Epoch: 139, Train Loss: 0.867\n",
      "Test Loss: 0.245, Test Acc: 94.34%\n",
      "Epoch: 140, Train Loss: 0.875\n",
      "Test Loss: 0.253, Test Acc: 94.69%\n",
      "Epoch: 141, Train Loss: 0.834\n",
      "Test Loss: 0.248, Test Acc: 94.65%\n",
      "Epoch: 142, Train Loss: 0.860\n",
      "Test Loss: 0.251, Test Acc: 94.74%\n",
      "Epoch: 143, Train Loss: 0.864\n",
      "Test Loss: 0.251, Test Acc: 94.77%\n",
      "Epoch: 144, Train Loss: 0.814\n",
      "Test Loss: 0.269, Test Acc: 94.91%\n",
      "Epoch: 145, Train Loss: 0.865\n",
      "Test Loss: 0.329, Test Acc: 94.82%\n",
      "Epoch: 146, Train Loss: 0.865\n",
      "Test Loss: 0.252, Test Acc: 94.88%\n",
      "Epoch: 147, Train Loss: 0.801\n",
      "Test Loss: 0.255, Test Acc: 95.00%\n",
      "Epoch: 148, Train Loss: 0.788\n",
      "Test Loss: 0.316, Test Acc: 94.98%\n",
      "Epoch: 149, Train Loss: 0.842\n",
      "Test Loss: 0.282, Test Acc: 94.90%\n",
      "Epoch: 150, Train Loss: 0.853\n",
      "Test Loss: 0.288, Test Acc: 94.90%\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>test_accuracy</td><td>▁▅▆▅▅▆▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▆▇▇▇▇▇▇▇▇█▇████████</td></tr><tr><td>test_loss</td><td>█▇▅▇▅▄▄▄▄▄▅▄▃▃▄▃▃▄▃▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_accuracy</td><td>95</td></tr><tr><td>epoch</td><td>150</td></tr><tr><td>final_accuracy</td><td>94.9</td></tr><tr><td>test_accuracy</td><td>94.9</td></tr><tr><td>test_loss</td><td>0.28779</td></tr><tr><td>train_loss</td><td>0.85325</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sweet-sweep-4</strong> at: <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/exoic84s' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/exoic84s</a><br> View project at: <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250307_145507-exoic84s/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mv0tuoii with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcolor_jitter_brightness: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thorizontal_flip: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmixup_alpha: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 150\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_crop: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_erasing_prob: 0.2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/JL/Desktop/DL_Proj/wandb/run-20250307_160835-mv0tuoii</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/mv0tuoii' target=\"_blank\">crisp-sweep-5</a></strong> to <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/sweeps/knffhnpf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/mv0tuoii' target=\"_blank\">https://wandb.ai/jl10897-new-york-university/cifar10-augmentation-sweep/runs/mv0tuoii</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple MPS for acceleration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JL/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/JL/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 2,797,610\n",
      "Trainable parameters: 2,797,610\n",
      "Epoch: 1, Train Loss: 2.154\n",
      "Test Loss: 1.725, Test Acc: 35.74%\n",
      "Epoch: 2, Train Loss: 1.927\n",
      "Test Loss: 1.430, Test Acc: 47.98%\n",
      "Epoch: 3, Train Loss: 1.732\n",
      "Test Loss: 1.302, Test Acc: 53.50%\n",
      "Epoch: 4, Train Loss: 1.586\n",
      "Test Loss: 1.021, Test Acc: 64.56%\n",
      "Epoch: 5, Train Loss: 1.450\n",
      "Test Loss: 1.076, Test Acc: 62.70%\n",
      "Epoch: 6, Train Loss: 1.393\n",
      "Test Loss: 0.965, Test Acc: 67.68%\n",
      "Epoch: 7, Train Loss: 1.312\n",
      "Test Loss: 0.805, Test Acc: 72.27%\n",
      "Epoch: 8, Train Loss: 1.277\n",
      "Test Loss: 1.126, Test Acc: 62.29%\n",
      "Epoch: 9, Train Loss: 1.221\n",
      "Test Loss: 0.721, Test Acc: 77.04%\n",
      "Epoch: 10, Train Loss: 1.212\n",
      "Test Loss: 0.738, Test Acc: 76.51%\n",
      "Epoch: 11, Train Loss: 1.206\n",
      "Test Loss: 0.702, Test Acc: 77.14%\n",
      "Epoch: 12, Train Loss: 1.178\n",
      "Test Loss: 0.779, Test Acc: 73.47%\n",
      "Epoch: 13, Train Loss: 1.182\n",
      "Test Loss: 0.811, Test Acc: 73.52%\n",
      "Epoch: 14, Train Loss: 1.141\n",
      "Test Loss: 0.807, Test Acc: 72.56%\n",
      "Epoch: 15, Train Loss: 1.124\n",
      "Test Loss: 0.877, Test Acc: 71.48%\n",
      "Epoch: 16, Train Loss: 1.160\n",
      "Test Loss: 0.785, Test Acc: 73.61%\n",
      "Epoch: 17, Train Loss: 1.141\n",
      "Test Loss: 0.746, Test Acc: 76.71%\n",
      "Epoch: 18, Train Loss: 1.101\n",
      "Test Loss: 0.628, Test Acc: 79.94%\n",
      "Epoch: 19, Train Loss: 1.138\n",
      "Test Loss: 0.694, Test Acc: 78.41%\n",
      "Epoch: 20, Train Loss: 1.089\n",
      "Test Loss: 0.681, Test Acc: 76.64%\n",
      "Epoch: 21, Train Loss: 1.146\n",
      "Test Loss: 0.708, Test Acc: 76.39%\n",
      "Epoch: 22, Train Loss: 1.119\n",
      "Test Loss: 0.636, Test Acc: 80.36%\n",
      "Epoch: 23, Train Loss: 1.109\n",
      "Test Loss: 0.683, Test Acc: 77.47%\n",
      "Epoch: 24, Train Loss: 1.092\n",
      "Test Loss: 0.871, Test Acc: 71.18%\n",
      "Epoch: 25, Train Loss: 1.088\n",
      "Test Loss: 0.624, Test Acc: 80.76%\n",
      "Epoch: 26, Train Loss: 1.075\n",
      "Test Loss: 0.746, Test Acc: 74.92%\n",
      "Epoch: 27, Train Loss: 1.080\n",
      "Test Loss: 0.636, Test Acc: 79.72%\n",
      "Epoch: 28, Train Loss: 1.091\n",
      "Test Loss: 0.781, Test Acc: 73.95%\n",
      "Epoch: 29, Train Loss: 1.072\n",
      "Test Loss: 0.636, Test Acc: 79.71%\n",
      "Epoch: 30, Train Loss: 1.086\n",
      "Test Loss: 0.627, Test Acc: 80.34%\n",
      "Epoch: 31, Train Loss: 1.128\n",
      "Test Loss: 0.619, Test Acc: 79.55%\n",
      "Epoch: 32, Train Loss: 1.074\n",
      "Test Loss: 0.758, Test Acc: 75.52%\n",
      "Epoch: 33, Train Loss: 1.065\n",
      "Test Loss: 0.839, Test Acc: 72.67%\n",
      "Epoch: 34, Train Loss: 1.044\n",
      "Test Loss: 0.798, Test Acc: 72.05%\n",
      "Epoch: 35, Train Loss: 1.083\n",
      "Test Loss: 0.635, Test Acc: 79.03%\n",
      "Epoch: 36, Train Loss: 1.079\n",
      "Test Loss: 0.588, Test Acc: 81.51%\n",
      "Epoch: 37, Train Loss: 1.048\n",
      "Test Loss: 0.669, Test Acc: 77.36%\n",
      "Epoch: 38, Train Loss: 1.044\n",
      "Test Loss: 0.605, Test Acc: 80.73%\n",
      "Epoch: 39, Train Loss: 1.002\n",
      "Test Loss: 0.615, Test Acc: 80.09%\n",
      "Epoch: 40, Train Loss: 1.037\n",
      "Test Loss: 0.577, Test Acc: 81.17%\n",
      "Epoch: 41, Train Loss: 1.032\n",
      "Test Loss: 0.531, Test Acc: 83.03%\n",
      "Epoch: 42, Train Loss: 1.043\n",
      "Test Loss: 0.639, Test Acc: 79.04%\n",
      "Epoch: 43, Train Loss: 1.048\n",
      "Test Loss: 0.632, Test Acc: 78.96%\n",
      "Epoch: 44, Train Loss: 1.014\n",
      "Test Loss: 0.725, Test Acc: 76.89%\n",
      "Epoch: 45, Train Loss: 1.041\n",
      "Test Loss: 0.598, Test Acc: 79.82%\n",
      "Epoch: 46, Train Loss: 1.016\n",
      "Test Loss: 0.598, Test Acc: 81.97%\n",
      "Epoch: 47, Train Loss: 1.054\n",
      "Test Loss: 0.509, Test Acc: 84.16%\n",
      "Epoch: 48, Train Loss: 1.041\n",
      "Test Loss: 0.595, Test Acc: 81.74%\n",
      "Epoch: 49, Train Loss: 0.992\n",
      "Test Loss: 0.569, Test Acc: 81.40%\n",
      "Epoch: 50, Train Loss: 1.005\n",
      "Test Loss: 0.551, Test Acc: 81.41%\n",
      "Epoch: 51, Train Loss: 1.032\n",
      "Test Loss: 0.576, Test Acc: 80.90%\n",
      "Epoch: 52, Train Loss: 1.020\n",
      "Test Loss: 0.567, Test Acc: 82.92%\n",
      "Epoch: 53, Train Loss: 1.010\n",
      "Test Loss: 0.691, Test Acc: 77.47%\n",
      "Epoch: 54, Train Loss: 1.014\n",
      "Test Loss: 0.543, Test Acc: 82.18%\n",
      "Epoch: 55, Train Loss: 1.014\n",
      "Test Loss: 0.640, Test Acc: 80.19%\n",
      "Epoch: 56, Train Loss: 0.997\n",
      "Test Loss: 0.558, Test Acc: 82.58%\n",
      "Epoch: 57, Train Loss: 1.002\n",
      "Test Loss: 0.560, Test Acc: 82.62%\n",
      "Epoch: 58, Train Loss: 0.999\n",
      "Test Loss: 0.512, Test Acc: 84.65%\n",
      "Epoch: 59, Train Loss: 0.975\n",
      "Test Loss: 0.541, Test Acc: 82.74%\n",
      "Epoch: 60, Train Loss: 0.989\n",
      "Test Loss: 0.591, Test Acc: 81.55%\n",
      "Epoch: 61, Train Loss: 0.955\n",
      "Test Loss: 0.491, Test Acc: 85.24%\n",
      "Epoch: 62, Train Loss: 0.994\n",
      "Test Loss: 0.639, Test Acc: 79.50%\n",
      "Epoch: 63, Train Loss: 0.983\n",
      "Test Loss: 0.556, Test Acc: 83.64%\n",
      "Epoch: 64, Train Loss: 0.963\n",
      "Test Loss: 0.639, Test Acc: 80.16%\n",
      "Epoch: 65, Train Loss: 0.966\n",
      "Test Loss: 0.571, Test Acc: 81.29%\n",
      "Epoch: 66, Train Loss: 0.947\n",
      "Test Loss: 0.473, Test Acc: 84.22%\n",
      "Epoch: 67, Train Loss: 0.926\n",
      "Test Loss: 0.585, Test Acc: 80.62%\n",
      "Epoch: 68, Train Loss: 0.956\n",
      "Test Loss: 0.463, Test Acc: 85.16%\n",
      "Epoch: 69, Train Loss: 0.960\n",
      "Test Loss: 0.456, Test Acc: 85.70%\n",
      "Epoch: 70, Train Loss: 0.975\n",
      "Test Loss: 0.556, Test Acc: 83.64%\n",
      "Epoch: 71, Train Loss: 0.932\n",
      "Test Loss: 0.482, Test Acc: 84.37%\n",
      "Epoch: 72, Train Loss: 0.941\n",
      "Test Loss: 0.466, Test Acc: 84.61%\n",
      "Epoch: 73, Train Loss: 0.902\n",
      "Test Loss: 0.497, Test Acc: 84.91%\n",
      "Epoch: 74, Train Loss: 0.972\n",
      "Test Loss: 0.530, Test Acc: 83.10%\n",
      "Epoch: 75, Train Loss: 0.929\n",
      "Test Loss: 0.505, Test Acc: 83.91%\n",
      "Epoch: 76, Train Loss: 0.971\n",
      "Test Loss: 0.463, Test Acc: 86.29%\n",
      "Epoch: 77, Train Loss: 0.944\n",
      "Test Loss: 0.506, Test Acc: 84.73%\n",
      "Epoch: 78, Train Loss: 0.898\n",
      "Test Loss: 0.492, Test Acc: 84.00%\n",
      "Epoch: 79, Train Loss: 0.938\n",
      "Test Loss: 0.608, Test Acc: 79.44%\n",
      "Epoch: 80, Train Loss: 0.920\n",
      "Test Loss: 0.519, Test Acc: 84.06%\n",
      "Epoch: 81, Train Loss: 0.922\n",
      "Test Loss: 0.422, Test Acc: 86.97%\n",
      "Epoch: 82, Train Loss: 0.929\n",
      "Test Loss: 0.454, Test Acc: 86.22%\n",
      "Epoch: 83, Train Loss: 0.914\n",
      "Test Loss: 0.447, Test Acc: 85.93%\n",
      "Epoch: 84, Train Loss: 0.927\n",
      "Test Loss: 0.520, Test Acc: 84.45%\n",
      "Epoch: 85, Train Loss: 0.881\n",
      "Test Loss: 0.501, Test Acc: 87.23%\n",
      "Epoch: 86, Train Loss: 0.915\n",
      "Test Loss: 0.446, Test Acc: 86.89%\n",
      "Epoch: 87, Train Loss: 0.913\n",
      "Test Loss: 0.479, Test Acc: 85.12%\n",
      "Epoch: 88, Train Loss: 0.900\n",
      "Test Loss: 0.372, Test Acc: 87.83%\n",
      "Epoch: 89, Train Loss: 0.887\n",
      "Test Loss: 0.432, Test Acc: 86.13%\n",
      "Epoch: 90, Train Loss: 0.871\n",
      "Test Loss: 0.457, Test Acc: 86.46%\n",
      "Epoch: 91, Train Loss: 0.881\n",
      "Test Loss: 0.445, Test Acc: 86.86%\n",
      "Epoch: 92, Train Loss: 0.866\n",
      "Test Loss: 0.418, Test Acc: 87.91%\n",
      "Epoch: 93, Train Loss: 0.853\n",
      "Test Loss: 0.407, Test Acc: 87.54%\n",
      "Epoch: 94, Train Loss: 0.866\n",
      "Test Loss: 0.382, Test Acc: 87.62%\n",
      "Epoch: 95, Train Loss: 0.895\n",
      "Test Loss: 0.398, Test Acc: 88.57%\n",
      "Epoch: 96, Train Loss: 0.884\n",
      "Test Loss: 0.420, Test Acc: 87.19%\n",
      "Epoch: 97, Train Loss: 0.853\n",
      "Test Loss: 0.346, Test Acc: 89.15%\n",
      "Epoch: 98, Train Loss: 0.822\n",
      "Test Loss: 0.395, Test Acc: 89.24%\n",
      "Epoch: 99, Train Loss: 0.823\n",
      "Test Loss: 0.386, Test Acc: 89.20%\n",
      "Epoch: 100, Train Loss: 0.874\n",
      "Test Loss: 0.387, Test Acc: 87.59%\n",
      "Epoch: 101, Train Loss: 0.842\n",
      "Test Loss: 0.399, Test Acc: 88.71%\n",
      "Epoch: 102, Train Loss: 0.836\n",
      "Test Loss: 0.390, Test Acc: 88.91%\n",
      "Epoch: 103, Train Loss: 0.787\n",
      "Test Loss: 0.354, Test Acc: 89.14%\n",
      "Epoch: 104, Train Loss: 0.801\n",
      "Test Loss: 0.354, Test Acc: 89.52%\n",
      "Epoch: 105, Train Loss: 0.818\n",
      "Test Loss: 0.435, Test Acc: 87.13%\n",
      "Epoch: 106, Train Loss: 0.789\n",
      "Test Loss: 0.325, Test Acc: 90.04%\n",
      "Epoch: 107, Train Loss: 0.808\n",
      "Test Loss: 0.312, Test Acc: 90.95%\n",
      "Epoch: 108, Train Loss: 0.807\n",
      "Test Loss: 0.345, Test Acc: 90.23%\n",
      "Epoch: 109, Train Loss: 0.796\n",
      "Test Loss: 0.324, Test Acc: 89.59%\n",
      "Epoch: 110, Train Loss: 0.798\n",
      "Test Loss: 0.346, Test Acc: 90.10%\n",
      "Epoch: 111, Train Loss: 0.775\n",
      "Test Loss: 0.334, Test Acc: 90.04%\n",
      "Epoch: 112, Train Loss: 0.780\n",
      "Test Loss: 0.306, Test Acc: 90.62%\n",
      "Epoch: 113, Train Loss: 0.745\n",
      "Test Loss: 0.270, Test Acc: 91.86%\n"
     ]
    }
   ],
   "source": [
    "# 6. Sweep Configuration and Execution\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "    \n",
    "    wandb.login()\n",
    "    sweep_config = {\n",
    "        \"method\": \"grid\",\n",
    "        \"metric\": {\"name\": \"test_accuracy\", \"goal\": \"maximize\"},\n",
    "        \"parameters\": {\n",
    "            \"mixup_alpha\": {\"values\": [0.2,0.4]},\n",
    "            \"random_crop\": {\"values\": [True]},\n",
    "            \"horizontal_flip\": {\"values\": [True]}, #Horizontal Flip 确实有帮助！\n",
    "            \"color_jitter_brightness\": {\"values\": [0.2,0.4]},\n",
    "            \"random_erasing_prob\": {\"values\": [0.2,0.4]},\n",
    "            \"num_epochs\": {\"values\": [150]}  # Add this line to set num_epochs\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"cifar10-augmentation-sweep\")\n",
    "    # wandb.agent(sweep_id, train, count=10)  # count=20 表示运行20次实验\n",
    "    wandb.agent(sweep_id, train)  # 不需要count参数，会自动运行所有组合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Prediction on Test Dataset\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "import os\n",
    "import glob\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, idx  # Return index as a placeholder for label\n",
    "\n",
    "def predict_test_dataset():\n",
    "    # Load the test dataset\n",
    "    with open('/Users/JL/Desktop/DL_Proj/cifar_test_nolabel.pkl', 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    \n",
    "    # Define the same transformations used for testing\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.2470, 0.2435, 0.2616]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Create test dataset and dataloader\n",
    "    test_dataset = CustomTestDataset(test_data, transform=test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "    \n",
    "    # Set device\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(f\"Using Apple MPS for acceleration\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using CUDA for acceleration\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"Using CPU (no GPU/MPS available)\")\n",
    "    \n",
    "    # Initialize model using the same architecture as in training\n",
    "    model = get_resnet18_model(num_classes=10).to(device)\n",
    "    \n",
    "    # Try to find the best model from all runs\n",
    "    try:\n",
    "        api = wandb.Api()\n",
    "        runs = api.runs(\"cifar10-augmentation-sweep\")\n",
    "        best_run = None\n",
    "        best_accuracy = 0\n",
    "        \n",
    "        for run in runs:\n",
    "            if run.state == \"finished\" and run.summary.get(\"best_accuracy\", 0) > best_accuracy:\n",
    "                best_accuracy = run.summary.get(\"best_accuracy\", 0)\n",
    "                best_run = run\n",
    "        \n",
    "        if best_run:\n",
    "            run_id = best_run.id\n",
    "            best_model_path = f'best_model_{run_id}.pth'\n",
    "            print(f\"Using best model from run {best_run.name} with accuracy {best_accuracy:.2f}%\")\n",
    "            \n",
    "            try:\n",
    "                # Try to download the model file\n",
    "                model_file = best_run.file(best_model_path).download(replace=True)\n",
    "                model.load_state_dict(torch.load(model_file.name, map_location=device))\n",
    "                print(f\"Successfully loaded model from wandb\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading model from wandb: {e}\")\n",
    "                \n",
    "                # Try to find the model locally\n",
    "                if os.path.exists(best_model_path):\n",
    "                    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "                    print(f\"Loaded model from local file: {best_model_path}\")\n",
    "                else:\n",
    "                    print(f\"Could not find model file: {best_model_path}\")\n",
    "                    \n",
    "                    # Try to find any best model file locally\n",
    "                    best_models = glob.glob('best_model_*.pth')\n",
    "                    if best_models:\n",
    "                        latest_model = max(best_models, key=os.path.getctime)\n",
    "                        model.load_state_dict(torch.load(latest_model, map_location=device))\n",
    "                        print(f\"Loaded most recent local model: {latest_model}\")\n",
    "                    else:\n",
    "                        print(\"No model files found locally\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing wandb: {e}\")\n",
    "        \n",
    "        # Try to find any best model file locally\n",
    "        best_models = glob.glob('best_model_*.pth')\n",
    "        if best_models:\n",
    "            latest_model = max(best_models, key=os.path.getctime)\n",
    "            model.load_state_dict(torch.load(latest_model, map_location=device))\n",
    "            print(f\"Loaded most recent local model: {latest_model}\")\n",
    "        else:\n",
    "            print(\"WARNING: Could not load any model. Using untrained model for predictions.\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images, indices in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            for idx, pred in zip(indices.tolist(), preds.tolist()):\n",
    "                predictions.append((idx, pred))\n",
    "    \n",
    "    # Sort predictions by index\n",
    "    predictions.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Create submission DataFrame\n",
    "    submission_df = pd.DataFrame(predictions, columns=['ID', 'Labels'])\n",
    "    \n",
    "    # Save to CSV\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print(f\"Predictions saved to submission.csv\")\n",
    "\n",
    "# Run the prediction function\n",
    "predict_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to call prediction function...\n",
      "Starting prediction function...\n",
      "Loading test data...\n",
      "Test data loaded successfully!\n",
      "Creating dataset and dataloader...\n",
      "Dataset created with 10000 samples\n",
      "Setting up device...\n",
      "Using Apple MPS for acceleration\n",
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/JL/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/JL/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized\n",
      "Loading model weights...\n",
      "Successfully loaded model from: /Users/JL/Desktop/DL_Proj/final_model_ku8uu15z.pth\n",
      "Model set to evaluation mode\n",
      "Making predictions...\n",
      "Processing batch 1/100\n",
      "Processing batch 2/100\n",
      "Processing batch 3/100\n",
      "Processing batch 4/100\n",
      "Processing batch 5/100\n",
      "Processing batch 6/100\n",
      "Processing batch 7/100\n",
      "Processing batch 8/100\n",
      "Processing batch 9/100\n",
      "Processing batch 10/100\n",
      "Processing batch 11/100\n",
      "Processing batch 12/100\n",
      "Processing batch 13/100\n",
      "Processing batch 14/100\n",
      "Processing batch 15/100\n",
      "Processing batch 16/100\n",
      "Processing batch 17/100\n",
      "Processing batch 18/100\n",
      "Processing batch 19/100\n",
      "Processing batch 20/100\n",
      "Processing batch 21/100\n",
      "Processing batch 22/100\n",
      "Processing batch 23/100\n",
      "Processing batch 24/100\n",
      "Processing batch 25/100\n",
      "Processing batch 26/100\n",
      "Processing batch 27/100\n",
      "Processing batch 28/100\n",
      "Processing batch 29/100\n",
      "Processing batch 30/100\n",
      "Processing batch 31/100\n",
      "Processing batch 32/100\n",
      "Processing batch 33/100\n",
      "Processing batch 34/100\n",
      "Processing batch 35/100\n",
      "Processing batch 36/100\n",
      "Processing batch 37/100\n",
      "Processing batch 38/100\n",
      "Processing batch 39/100\n",
      "Processing batch 40/100\n",
      "Processing batch 41/100\n",
      "Processing batch 42/100\n",
      "Processing batch 43/100\n",
      "Processing batch 44/100\n",
      "Processing batch 45/100\n",
      "Processing batch 46/100\n",
      "Processing batch 47/100\n",
      "Processing batch 48/100\n",
      "Processing batch 49/100\n",
      "Processing batch 50/100\n",
      "Processing batch 51/100\n",
      "Processing batch 52/100\n",
      "Processing batch 53/100\n",
      "Processing batch 54/100\n",
      "Processing batch 55/100\n",
      "Processing batch 56/100\n",
      "Processing batch 57/100\n",
      "Processing batch 58/100\n",
      "Processing batch 59/100\n",
      "Processing batch 60/100\n",
      "Processing batch 61/100\n",
      "Processing batch 62/100\n",
      "Processing batch 63/100\n",
      "Processing batch 64/100\n",
      "Processing batch 65/100\n",
      "Processing batch 66/100\n",
      "Processing batch 67/100\n",
      "Processing batch 68/100\n",
      "Processing batch 69/100\n",
      "Processing batch 70/100\n",
      "Processing batch 71/100\n",
      "Processing batch 72/100\n",
      "Processing batch 73/100\n",
      "Processing batch 74/100\n",
      "Processing batch 75/100\n",
      "Processing batch 76/100\n",
      "Processing batch 77/100\n",
      "Processing batch 78/100\n",
      "Processing batch 79/100\n",
      "Processing batch 80/100\n",
      "Processing batch 81/100\n",
      "Processing batch 82/100\n",
      "Processing batch 83/100\n",
      "Processing batch 84/100\n",
      "Processing batch 85/100\n",
      "Processing batch 86/100\n",
      "Processing batch 87/100\n",
      "Processing batch 88/100\n",
      "Processing batch 89/100\n",
      "Processing batch 90/100\n",
      "Processing batch 91/100\n",
      "Processing batch 92/100\n",
      "Processing batch 93/100\n",
      "Processing batch 94/100\n",
      "Processing batch 95/100\n",
      "Processing batch 96/100\n",
      "Processing batch 97/100\n",
      "Processing batch 98/100\n",
      "Processing batch 99/100\n",
      "Processing batch 100/100\n",
      "Creating submission file...\n",
      "Predictions saved to submission.csv\n",
      "Prediction function completed successfully!\n",
      "Prediction function call completed.\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import os\n",
    "# import glob\n",
    "# from PIL import Image\n",
    "\n",
    "# def predict_test_dataset(model_path=None):\n",
    "#     print(\"Starting prediction function...\")\n",
    "    \n",
    "#     # Load the test dataset\n",
    "#     print(\"Loading test data...\")\n",
    "#     with open('/Users/JL/Desktop/DL_Proj/cifar_test_nolabel.pkl', 'rb') as f:\n",
    "#         test_data = pickle.load(f)\n",
    "#     print(\"Test data loaded successfully!\")\n",
    "    \n",
    "#     # Define the same transformations used for testing\n",
    "#     test_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(\n",
    "#             mean=[0.4914, 0.4822, 0.4465],\n",
    "#             std=[0.2470, 0.2435, 0.2616]\n",
    "#         )\n",
    "#     ])\n",
    "    \n",
    "#     # Create a proper dataset based on the actual structure\n",
    "#     class CIFARTestDataset(Dataset):\n",
    "#         def __init__(self, data_dict, transform=None):\n",
    "#             self.images = data_dict[b'data']  # Already in shape (10000, 32, 32, 3)\n",
    "#             self.ids = data_dict[b'ids']\n",
    "#             self.transform = transform\n",
    "            \n",
    "#         def __len__(self):\n",
    "#             return len(self.images)\n",
    "        \n",
    "#         def __getitem__(self, idx):\n",
    "#             image = self.images[idx]\n",
    "#             image_id = int(self.ids[idx])  # Convert to int for DataFrame\n",
    "            \n",
    "#             # Convert to PIL Image for transforms\n",
    "#             image = Image.fromarray(image)\n",
    "            \n",
    "#             if self.transform:\n",
    "#                 image = self.transform(image)\n",
    "                \n",
    "#             return image, image_id\n",
    "    \n",
    "#     # Create test dataset and dataloader\n",
    "#     print(\"Creating dataset and dataloader...\")\n",
    "#     test_dataset = CIFARTestDataset(test_data, transform=test_transform)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "#     print(f\"Dataset created with {len(test_dataset)} samples\")\n",
    "    \n",
    "#     # Set device\n",
    "#     print(\"Setting up device...\")\n",
    "#     if torch.backends.mps.is_available():\n",
    "#         device = torch.device(\"mps\")\n",
    "#         print(f\"Using Apple MPS for acceleration\")\n",
    "#     elif torch.cuda.is_available():\n",
    "#         device = torch.device(\"cuda\")\n",
    "#         print(f\"Using CUDA for acceleration\")\n",
    "#     else:\n",
    "#         device = torch.device(\"cpu\")\n",
    "#         print(f\"Using CPU (no GPU/MPS available)\")\n",
    "    \n",
    "#     # Initialize model using the same architecture as in training\n",
    "#     print(\"Initializing model...\")\n",
    "#     model = get_resnet18_model(num_classes=10).to(device)\n",
    "#     print(\"Model initialized\")\n",
    "    \n",
    "#     # Load the specified model if provided\n",
    "#     print(\"Loading model weights...\")\n",
    "#     if model_path and os.path.exists(model_path):\n",
    "#         model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "#         print(f\"Successfully loaded model from: {model_path}\")\n",
    "#     else:\n",
    "#         # If no specific model is provided, try to find the best model\n",
    "#         if model_path:\n",
    "#             print(f\"Specified model {model_path} not found.\")\n",
    "        \n",
    "#         # Try to find any best model file locally\n",
    "#         best_models = glob.glob('best_model_*.pth')\n",
    "#         if best_models:\n",
    "#             latest_model = max(best_models, key=os.path.getctime)\n",
    "#             model.load_state_dict(torch.load(latest_model, map_location=device))\n",
    "#             print(f\"Loaded most recent local model: {latest_model}\")\n",
    "#         else:\n",
    "#             print(\"WARNING: Could not load any model. Using untrained model for predictions.\")\n",
    "    \n",
    "#     # Set model to evaluation mode\n",
    "#     model.eval()\n",
    "#     print(\"Model set to evaluation mode\")\n",
    "    \n",
    "#     # Make predictions\n",
    "#     print(\"Making predictions...\")\n",
    "#     predictions = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (images, ids) in enumerate(test_loader):\n",
    "#             print(f\"Processing batch {batch_idx+1}/{len(test_loader)}\")\n",
    "#             images = images.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "#             for id_val, pred in zip(ids.numpy(), preds.cpu().numpy()):\n",
    "#                 predictions.append((id_val, pred))\n",
    "    \n",
    "#     # Create submission DataFrame\n",
    "#     print(\"Creating submission file...\")\n",
    "#     submission_df = pd.DataFrame(predictions, columns=['ID', 'Labels'])\n",
    "    \n",
    "#     # Save to CSV\n",
    "#     submission_df.to_csv('submission.csv', index=False)\n",
    "#     print(f\"Predictions saved to submission.csv\")\n",
    "#     print(\"Prediction function completed successfully!\")\n",
    "\n",
    "# # Call the function with your model path\n",
    "# print(\"About to call prediction function...\")\n",
    "# predict_test_dataset(\"/Users/JL/Desktop/DL_Proj/final_model_ku8uu15z.pth\")\n",
    "# print(\"Prediction function call completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting debug function...\n",
      "Attempting to load test data...\n",
      "Test data loaded successfully!\n",
      "Test data type: <class 'dict'>\n",
      "Test data keys: [b'data', b'ids']\n",
      "Debug function completed successfully!\n",
      "\n",
      "Examining test data further...\n",
      "Key: b'data', Type: <class 'numpy.ndarray'>\n",
      "Length/Shape: (10000, 32, 32, 3)\n",
      "Key: b'ids', Type: <class 'numpy.ndarray'>\n",
      "Length/Shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import os\n",
    "# import glob\n",
    "# from PIL import Image\n",
    "\n",
    "# def debug_test_dataset():\n",
    "#     print(\"Starting debug function...\")\n",
    "    \n",
    "#     try:\n",
    "#         # Load the test dataset\n",
    "#         print(\"Attempting to load test data...\")\n",
    "#         with open('/Users/JL/Desktop/DL_Proj/cifar_test_nolabel.pkl', 'rb') as f:\n",
    "#             test_data = pickle.load(f)\n",
    "#         print(\"Test data loaded successfully!\")\n",
    "        \n",
    "#         # Print test data structure for debugging\n",
    "#         print(f\"Test data type: {type(test_data)}\")\n",
    "        \n",
    "#         if isinstance(test_data, dict):\n",
    "#             print(f\"Test data keys: {list(test_data.keys())}\")\n",
    "#         elif isinstance(test_data, list):\n",
    "#             print(f\"Test data is a list with {len(test_data)} items\")\n",
    "#         else:\n",
    "#             print(f\"Test data is of type {type(test_data)}\")\n",
    "            \n",
    "#         print(\"Debug function completed successfully!\")\n",
    "#         return test_data\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in debug function: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "# # Call the debug function\n",
    "# test_data = debug_test_dataset()\n",
    "\n",
    "# # If we got data, try to examine it further\n",
    "# if test_data is not None:\n",
    "#     print(\"\\nExamining test data further...\")\n",
    "#     if isinstance(test_data, dict):\n",
    "#         for key in test_data.keys():\n",
    "#             print(f\"Key: {key}, Type: {type(test_data[key])}\")\n",
    "#             if isinstance(test_data[key], (list, tuple, np.ndarray)):\n",
    "#                 print(f\"Length/Shape: {len(test_data[key]) if isinstance(test_data[key], (list, tuple)) else test_data[key].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
