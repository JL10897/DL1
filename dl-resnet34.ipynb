{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.transforms import autoaugment\n",
    "from PIL import Image\n",
    "import time\n",
    "import pickle\n",
    "import wandb\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1. Data Loading and Preprocessing with configurable augmentation\n",
    "cifar10_dir = 'deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/'\n",
    "def load_cifar_batch(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "from PIL import Image\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, idx  # Return index as a placeholder for label\n",
    "class CustomCIFAR10Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images            # e.g. a NumPy array of shape (N, 32, 32, 3)\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]         # This is a NumPy array\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert to PIL if your transforms expect PIL images:\n",
    "        # (If your transforms are purely tensor-based and include\n",
    "        #  transforms.ToTensor() as the first step, you can skip this\n",
    "        #  and let ToTensor() handle the NumPy array directly.)\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    \n",
    "def get_data_loaders(batch_size=128, num_workers=0, config=None):\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.4914, 0.4822, 0.4465],\n",
    "        std=[0.2470, 0.2435, 0.2616]\n",
    "    )\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "    \n",
    "    # Use wandb config to control the parameters\n",
    "    train_transform_list = [transforms.ToTensor(), normalize]\n",
    "    if config.random_crop:\n",
    "        train_transform_list.insert(0, transforms.RandomCrop(32, padding=4))\n",
    "    if config.horizontal_flip:\n",
    "        train_transform_list.insert(0, transforms.RandomHorizontalFlip())\n",
    "    train_transform_list.insert(0, transforms.ColorJitter(\n",
    "        brightness=config.color_jitter_brightness,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.1\n",
    "    ))\n",
    "    train_transform_list.insert(0, autoaugment.AutoAugment(policy=autoaugment.AutoAugmentPolicy.CIFAR10))\n",
    "    train_transform_list.append(transforms.RandomErasing(\n",
    "        p=config.random_erasing_prob,\n",
    "        scale=(0.02, 0.33),\n",
    "        ratio=(0.3, 3.3),\n",
    "        value=0\n",
    "    ))\n",
    "    \n",
    "    train_transform = transforms.Compose(train_transform_list)\n",
    "\n",
    "    # Specify the directory containing CIFAR-10 batches\n",
    "    cifar10_dir = 'deep-learning-spring-2025-project-1/cifar-10-python/cifar-10-batches-py/'\n",
    "\n",
    "    # Load metadata (labels)\n",
    "    meta_data_dict = load_cifar_batch(os.path.join(cifar10_dir, 'batches.meta'))\n",
    "    label_names = [label.decode('utf-8') for label in meta_data_dict[b'label_names']]\n",
    "\n",
    "    # Load training data\n",
    "    train_data = []\n",
    "    train_labels = []\n",
    "    for i in range(1, 6):\n",
    "        batch = load_cifar_batch(os.path.join(cifar10_dir, f'data_batch_{i}'))\n",
    "        train_data.append(batch[b'data'])\n",
    "        train_labels += batch[b'labels']\n",
    "\n",
    "    train_data = np.vstack(train_data).reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)  # Convert to HWC format\n",
    "    train_labels = np.array(train_labels)\n",
    "\n",
    "    # Load test data\n",
    "    batch_test_dict = load_cifar_batch(os.path.join(cifar10_dir, 'test_batch'))\n",
    "    val_images = batch_test_dict[b'data'].reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1)\n",
    "    val_labels = np.array(batch_test_dict[b'labels'])\n",
    "    \n",
    "    #get the datasets\n",
    "    train_dataset = CustomCIFAR10Dataset(train_data, train_labels, transform=train_transform)\n",
    "    test_dataset = CustomCIFAR10Dataset(val_images, val_labels, transform=test_transform)\n",
    "\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=True, \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Definition (unchanged)\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    return total_params, trainable_params\n",
    "\n",
    "# def get_resnet18_model(num_classes=10, pretrained=False):\n",
    "#     model = models.resnet18(weights=weights)\n",
    "#     model.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.bn1 = nn.BatchNorm2d(32)\n",
    "#     model.maxpool = nn.Identity()\n",
    "    \n",
    "#     model.layer1[0].conv1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.layer1[0].conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.layer1[0].bn1 = nn.BatchNorm2d(32)\n",
    "#     model.layer1[0].bn2 = nn.BatchNorm2d(32)\n",
    "#     model.layer1[1].conv1 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.layer1[1].conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.layer1[1].bn1 = nn.BatchNorm2d(32)\n",
    "#     model.layer1[1].bn2 = nn.BatchNorm2d(32)\n",
    "    \n",
    "#     model.layer2[0].conv1 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "#     model.layer2[0].conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.layer2[0].bn1 = nn.BatchNorm2d(64)\n",
    "#     model.layer2[0].bn2 = nn.BatchNorm2d(64)\n",
    "#     model.layer2[0].downsample[0] = nn.Conv2d(32, 64, kernel_size=1, stride=2, bias=False)\n",
    "#     model.layer2[0].downsample[1] = nn.BatchNorm2d(64)\n",
    "#     model.layer2[1].conv1 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.layer2[1].conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.layer2[1].bn1 = nn.BatchNorm2d(64)\n",
    "#     model.layer2[1].bn2 = nn.BatchNorm2d(64)\n",
    "    \n",
    "#     model.layer3[0].conv1 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "#     model.layer3[0].conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.layer3[0].bn1 = nn.BatchNorm2d(128)\n",
    "#     model.layer3[0].bn2 = nn.BatchNorm2d(128)\n",
    "#     model.layer3[0].downsample[0] = nn.Conv2d(64, 128, kernel_size=1, stride=2, bias=False)\n",
    "#     model.layer3[0].downsample[1] = nn.BatchNorm2d(128)\n",
    "#     model.layer3[1].conv1 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.layer3[1].conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.layer3[1].bn1 = nn.BatchNorm2d(128)\n",
    "#     model.layer3[1].bn2 = nn.BatchNorm2d(128)\n",
    "    \n",
    "#     model.layer4[0].conv1 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "#     model.layer4[0].conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.layer4[0].bn1 = nn.BatchNorm2d(256)\n",
    "#     model.layer4[0].bn2 = nn.BatchNorm2d(256)\n",
    "#     model.layer4[0].downsample[0] = nn.Conv2d(128, 256, kernel_size=1, stride=2, bias=False)\n",
    "#     model.layer4[0].downsample[1] = nn.BatchNorm2d(256)\n",
    "#     model.layer4[1].conv1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.layer4[1].conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#     model.layer4[1].bn1 = nn.BatchNorm2d(256)\n",
    "#     model.layer4[1].bn2 = nn.BatchNorm2d(256)\n",
    "\n",
    "    \n",
    "    # model.fc = nn.Linear(256, num_classes)\n",
    "    # return model\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "#modified resnet34 structure\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.init_conv = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.init_bn = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(32, 32, 3, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 64, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 128, 5, stride=2)\n",
    "        self.layer4 = self._make_layer(128, 256, 2, stride=2)\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, blocks, stride):\n",
    "        layers = [ResidualBlock(in_channels, out_channels, stride)]\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.init_conv(x)\n",
    "        out = self.init_bn(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avg_pool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Mixup Function (unchanged)\n",
    "def mixup_data(batch, targets, alpha=1.0, device='cpu'):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = batch.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * batch + (1 - lam) * batch[index]\n",
    "    y_a, y_b = targets, targets[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training and Evaluation Functions with wandb logging\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, use_mixup=False, mixup_alpha=1.0):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        if use_mixup:\n",
    "            inputs, targets_a, targets_b, lam = mixup_data(inputs, targets, alpha=mixup_alpha, device=device)\n",
    "            outputs = model(inputs)\n",
    "            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if not use_mixup:\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    epoch_time = time.time() - start_time\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    if not use_mixup:\n",
    "        epoch_acc = 100.0 * correct / total\n",
    "        wandb.log({\"train_loss\": epoch_loss, \"train_accuracy\": epoch_acc, \"epoch\": epoch})\n",
    "        print(f'Epoch: {epoch}, Train Loss: {epoch_loss:.3f}, Train Acc: {epoch_acc:.2f}%')\n",
    "        return epoch_loss, epoch_acc\n",
    "    else:\n",
    "        wandb.log({\"train_loss\": epoch_loss, \"epoch\": epoch})\n",
    "        print(f'Epoch: {epoch}, Train Loss: {epoch_loss:.3f}')\n",
    "        return epoch_loss, None\n",
    "    \n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_acc = 100.0 * correct / total\n",
    "    wandb.log({\"test_loss\": test_loss, \"test_accuracy\": test_acc})\n",
    "    print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
    "    \n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5. Main Training Function with MPS Support\n",
    "# def train(config=None):\n",
    "#     with wandb.init(config=config):\n",
    "#         config = wandb.config\n",
    "        \n",
    "#         # 设备检测：优先 MPS，然后 CUDA，最后 CPU\n",
    "#         if torch.backends.mps.is_available():\n",
    "#             device = torch.device(\"mps\")\n",
    "#             print(f\"Using Apple MPS for acceleration\")\n",
    "#         elif torch.cuda.is_available():\n",
    "#             device = torch.device(\"cuda\")\n",
    "#             print(f\"Using CUDA for acceleration\")\n",
    "#         else:\n",
    "#             device = torch.device(\"cpu\")\n",
    "#             print(f\"Using CPU (no GPU/MPS available)\")\n",
    "        \n",
    "#         # 获取数据加载器\n",
    "#         train_loader, test_loader = get_data_loaders(batch_size=128, config=config)\n",
    "        \n",
    "#         # 初始化模型并移动到指定设备\n",
    "#         model = get_resnet18_model(num_classes=10).to(device)\n",
    "#         count_parameters(model)\n",
    "        \n",
    "#         # 定义损失函数和优化器\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "        \n",
    "#         # 获取训练的总轮数\n",
    "#         num_epochs = config.num_epochs if hasattr(config, 'num_epochs') else 20\n",
    "        \n",
    "#         # 调整学习率调度器的最大周期为训练轮数\n",
    "#         scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "        \n",
    "#         # 训练循环\n",
    "#         for epoch in range(num_epochs):\n",
    "#             train_loss, _ = train_one_epoch(\n",
    "#                 model, train_loader, criterion, optimizer, device, epoch+1,\n",
    "#                 use_mixup=True, mixup_alpha=config.mixup_alpha\n",
    "#             )\n",
    "#             test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "#             scheduler.step()\n",
    "\n",
    "# 5. Main Training Function with MPS Support\n",
    "\n",
    "def train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        \n",
    "        # Get the unique run ID to use in filenames\n",
    "        run_id = wandb.run.id\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(\"Using CUDA for acceleration\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"Using CPU (no GPU available)\")\n",
    "        \n",
    "        # getting dataloaders\n",
    "        train_loader, test_loader = get_data_loaders(batch_size=128, config=config)\n",
    "        \n",
    "        # Initialize the model and move it to the selected device\n",
    "        model = ResNet18(num_classes=10).to(device)\n",
    "        count_parameters(model)\n",
    "        \n",
    "        # cruterion and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
    "        \n",
    "        # number of epoch we train\n",
    "        num_epochs = config.num_epochs if hasattr(config, 'num_epochs') else 20\n",
    "        \n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "        \n",
    "        # tracking the best model\n",
    "        best_acc = 0.0\n",
    "        \n",
    "        # Create model filenames with run ID\n",
    "        best_model_path = f'best_model_{run_id}.pth'\n",
    "        final_model_path = f'final_model_{run_id}.pth'\n",
    "        \n",
    "        # loop for train\n",
    "        for epoch in range(num_epochs):\n",
    "            train_loss, _ = train_one_epoch(\n",
    "                model, train_loader, criterion, optimizer, device, epoch+1,\n",
    "                use_mixup=True, mixup_alpha=config.mixup_alpha\n",
    "            )\n",
    "            test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "            scheduler.step()\n",
    "            \n",
    "            # make sure the best model\n",
    "            if test_acc > best_acc:\n",
    "                best_acc = test_acc\n",
    "                # save the model to local\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                # save to wandb\n",
    "                wandb.save(best_model_path)\n",
    "                \n",
    "                # Also log the best accuracy to wandb config for easy retrieval\n",
    "                wandb.run.summary['best_accuracy'] = best_acc\n",
    "        \n",
    "        # save the final model\n",
    "        torch.save(model.state_dict(), final_model_path)\n",
    "        wandb.save(final_model_path)\n",
    "        \n",
    "        # Log final test accuracy\n",
    "        wandb.run.summary['final_accuracy'] = test_acc\n",
    "        \n",
    "        # Also save a record of which model is best for this run\n",
    "        with open(f'model_info_{run_id}.txt', 'w') as f:\n",
    "            f.write(f\"Best model: {best_model_path}, Accuracy: {best_acc:.2f}%\\n\")\n",
    "            f.write(f\"Final model: {final_model_path}, Accuracy: {test_acc:.2f}%\\n\")\n",
    "            f.write(\"\\nRun Configuration:\\n\")\n",
    "            for key, value in config._items.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        wandb.save(f'model_info_{run_id}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: r9ye1bdz\n",
      "Sweep URL: https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep/sweeps/r9ye1bdz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xwg1cm8l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcolor_jitter_brightness: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thorizontal_flip: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmixup_alpha: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 150\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_crop: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_erasing_prob: 0.2\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/kp2653/proj1/wandb/run-20250313_202547-xwg1cm8l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep/runs/xwg1cm8l' target=\"_blank\">major-sweep-1</a></strong> to <a href='https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep/sweeps/r9ye1bdz' target=\"_blank\">https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep/sweeps/r9ye1bdz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep/sweeps/r9ye1bdz' target=\"_blank\">https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep/sweeps/r9ye1bdz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep/runs/xwg1cm8l' target=\"_blank\">https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep/runs/xwg1cm8l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA for acceleration\n",
      "Total parameters: 3,850,410\n",
      "Trainable parameters: 3,850,410\n",
      "Epoch: 1, Train Loss: 2.186\n",
      "Test Loss: 2.048, Test Acc: 33.32%\n",
      "Epoch: 2, Train Loss: 1.948\n",
      "Test Loss: 1.661, Test Acc: 36.98%\n",
      "Epoch: 3, Train Loss: 1.795\n",
      "Test Loss: 1.424, Test Acc: 49.53%\n",
      "Epoch: 4, Train Loss: 1.660\n",
      "Test Loss: 1.251, Test Acc: 55.60%\n",
      "Epoch: 5, Train Loss: 1.533\n",
      "Test Loss: 1.152, Test Acc: 60.27%\n",
      "Epoch: 6, Train Loss: 1.443\n",
      "Test Loss: 0.978, Test Acc: 65.70%\n",
      "Epoch: 7, Train Loss: 1.372\n",
      "Test Loss: 1.032, Test Acc: 65.86%\n",
      "Epoch: 8, Train Loss: 1.259\n",
      "Test Loss: 1.015, Test Acc: 65.94%\n",
      "Epoch: 9, Train Loss: 1.247\n",
      "Test Loss: 0.777, Test Acc: 74.04%\n",
      "Epoch: 10, Train Loss: 1.231\n",
      "Test Loss: 0.889, Test Acc: 70.18%\n",
      "Epoch: 11, Train Loss: 1.270\n",
      "Test Loss: 0.918, Test Acc: 69.22%\n",
      "Epoch: 12, Train Loss: 1.203\n",
      "Test Loss: 0.818, Test Acc: 73.79%\n",
      "Epoch: 13, Train Loss: 1.164\n",
      "Test Loss: 0.682, Test Acc: 76.56%\n",
      "Epoch: 14, Train Loss: 1.193\n",
      "Test Loss: 0.763, Test Acc: 75.04%\n",
      "Epoch: 15, Train Loss: 1.126\n",
      "Test Loss: 0.710, Test Acc: 77.37%\n",
      "Epoch: 16, Train Loss: 1.164\n",
      "Test Loss: 0.771, Test Acc: 74.59%\n",
      "Epoch: 17, Train Loss: 1.130\n",
      "Test Loss: 0.652, Test Acc: 79.11%\n",
      "Epoch: 18, Train Loss: 1.150\n",
      "Test Loss: 0.932, Test Acc: 67.73%\n",
      "Epoch: 19, Train Loss: 1.145\n",
      "Test Loss: 0.737, Test Acc: 76.84%\n",
      "Epoch: 20, Train Loss: 1.113\n",
      "Test Loss: 0.674, Test Acc: 79.39%\n",
      "Epoch: 21, Train Loss: 1.108\n",
      "Test Loss: 0.622, Test Acc: 79.57%\n",
      "Epoch: 22, Train Loss: 1.076\n",
      "Test Loss: 0.625, Test Acc: 78.80%\n",
      "Epoch: 23, Train Loss: 1.103\n",
      "Test Loss: 0.614, Test Acc: 80.78%\n",
      "Epoch: 24, Train Loss: 1.140\n",
      "Test Loss: 0.758, Test Acc: 75.84%\n",
      "Epoch: 25, Train Loss: 1.101\n",
      "Test Loss: 0.603, Test Acc: 82.10%\n",
      "Epoch: 26, Train Loss: 1.112\n",
      "Test Loss: 0.546, Test Acc: 82.17%\n",
      "Epoch: 27, Train Loss: 1.085\n",
      "Test Loss: 0.620, Test Acc: 79.75%\n",
      "Epoch: 28, Train Loss: 1.079\n",
      "Test Loss: 0.616, Test Acc: 80.58%\n",
      "Epoch: 29, Train Loss: 1.060\n",
      "Test Loss: 0.817, Test Acc: 72.51%\n",
      "Epoch: 30, Train Loss: 1.062\n",
      "Test Loss: 0.580, Test Acc: 80.98%\n",
      "Epoch: 31, Train Loss: 1.084\n",
      "Test Loss: 0.628, Test Acc: 79.65%\n",
      "Epoch: 32, Train Loss: 1.075\n",
      "Test Loss: 0.659, Test Acc: 78.15%\n",
      "Epoch: 33, Train Loss: 1.068\n",
      "Test Loss: 0.608, Test Acc: 80.96%\n",
      "Epoch: 34, Train Loss: 1.046\n",
      "Test Loss: 0.604, Test Acc: 79.26%\n",
      "Epoch: 35, Train Loss: 1.052\n",
      "Test Loss: 0.619, Test Acc: 79.69%\n",
      "Epoch: 36, Train Loss: 1.026\n",
      "Test Loss: 0.840, Test Acc: 74.03%\n",
      "Epoch: 37, Train Loss: 1.054\n",
      "Test Loss: 0.603, Test Acc: 81.45%\n",
      "Epoch: 38, Train Loss: 1.039\n",
      "Test Loss: 0.584, Test Acc: 83.16%\n",
      "Epoch: 39, Train Loss: 1.064\n",
      "Test Loss: 0.631, Test Acc: 80.65%\n",
      "Epoch: 40, Train Loss: 1.010\n",
      "Test Loss: 0.661, Test Acc: 78.91%\n",
      "Epoch: 41, Train Loss: 1.058\n",
      "Test Loss: 0.671, Test Acc: 76.94%\n",
      "Epoch: 42, Train Loss: 1.032\n",
      "Test Loss: 0.626, Test Acc: 80.54%\n",
      "Epoch: 43, Train Loss: 1.051\n",
      "Test Loss: 0.812, Test Acc: 73.80%\n",
      "Epoch: 44, Train Loss: 1.041\n",
      "Test Loss: 0.717, Test Acc: 76.46%\n",
      "Epoch: 45, Train Loss: 1.047\n",
      "Test Loss: 0.730, Test Acc: 76.52%\n",
      "Epoch: 46, Train Loss: 1.020\n",
      "Test Loss: 0.573, Test Acc: 81.32%\n",
      "Epoch: 47, Train Loss: 1.035\n",
      "Test Loss: 0.497, Test Acc: 85.27%\n",
      "Epoch: 48, Train Loss: 1.014\n",
      "Test Loss: 0.481, Test Acc: 85.92%\n",
      "Epoch: 49, Train Loss: 1.043\n",
      "Test Loss: 0.694, Test Acc: 77.55%\n",
      "Epoch: 50, Train Loss: 0.990\n",
      "Test Loss: 0.919, Test Acc: 70.04%\n",
      "Epoch: 51, Train Loss: 1.005\n",
      "Test Loss: 0.458, Test Acc: 85.84%\n",
      "Epoch: 52, Train Loss: 0.999\n",
      "Test Loss: 0.500, Test Acc: 84.15%\n",
      "Epoch: 53, Train Loss: 0.983\n",
      "Test Loss: 0.622, Test Acc: 81.28%\n",
      "Epoch: 54, Train Loss: 0.986\n",
      "Test Loss: 0.578, Test Acc: 80.99%\n",
      "Epoch: 55, Train Loss: 1.008\n",
      "Test Loss: 0.601, Test Acc: 81.56%\n",
      "Epoch: 56, Train Loss: 0.968\n",
      "Test Loss: 0.540, Test Acc: 85.25%\n",
      "Epoch: 57, Train Loss: 0.995\n",
      "Test Loss: 0.527, Test Acc: 82.64%\n",
      "Epoch: 58, Train Loss: 0.980\n",
      "Test Loss: 0.632, Test Acc: 79.08%\n",
      "Epoch: 59, Train Loss: 0.958\n",
      "Test Loss: 0.512, Test Acc: 83.70%\n",
      "Epoch: 60, Train Loss: 0.979\n",
      "Test Loss: 0.525, Test Acc: 83.41%\n",
      "Epoch: 61, Train Loss: 0.999\n",
      "Test Loss: 0.632, Test Acc: 80.62%\n",
      "Epoch: 62, Train Loss: 0.983\n",
      "Test Loss: 0.568, Test Acc: 81.77%\n",
      "Epoch: 63, Train Loss: 0.967\n",
      "Test Loss: 0.577, Test Acc: 83.60%\n",
      "Epoch: 64, Train Loss: 0.955\n",
      "Test Loss: 0.634, Test Acc: 80.15%\n",
      "Epoch: 65, Train Loss: 0.936\n",
      "Test Loss: 0.620, Test Acc: 80.07%\n",
      "Epoch: 66, Train Loss: 0.931\n",
      "Test Loss: 0.477, Test Acc: 86.70%\n",
      "Epoch: 67, Train Loss: 0.957\n",
      "Test Loss: 0.532, Test Acc: 84.47%\n",
      "Epoch: 68, Train Loss: 0.960\n",
      "Test Loss: 0.508, Test Acc: 84.92%\n",
      "Epoch: 69, Train Loss: 0.963\n",
      "Test Loss: 0.533, Test Acc: 84.52%\n",
      "Epoch: 70, Train Loss: 0.936\n",
      "Test Loss: 0.437, Test Acc: 87.97%\n",
      "Epoch: 71, Train Loss: 0.932\n",
      "Test Loss: 0.525, Test Acc: 82.68%\n",
      "Epoch: 72, Train Loss: 0.935\n",
      "Test Loss: 0.465, Test Acc: 86.42%\n",
      "Epoch: 73, Train Loss: 0.939\n",
      "Test Loss: 0.501, Test Acc: 86.54%\n",
      "Epoch: 74, Train Loss: 0.924\n",
      "Test Loss: 0.453, Test Acc: 87.10%\n",
      "Epoch: 75, Train Loss: 0.959\n",
      "Test Loss: 0.463, Test Acc: 86.30%\n",
      "Epoch: 76, Train Loss: 0.927\n",
      "Test Loss: 0.403, Test Acc: 87.03%\n",
      "Epoch: 77, Train Loss: 0.892\n",
      "Test Loss: 0.434, Test Acc: 87.49%\n",
      "Epoch: 78, Train Loss: 0.901\n",
      "Test Loss: 0.447, Test Acc: 87.03%\n",
      "Epoch: 79, Train Loss: 0.935\n",
      "Test Loss: 0.432, Test Acc: 87.15%\n",
      "Epoch: 80, Train Loss: 0.930\n",
      "Test Loss: 0.485, Test Acc: 86.28%\n",
      "Epoch: 81, Train Loss: 0.904\n",
      "Test Loss: 0.462, Test Acc: 85.35%\n",
      "Epoch: 82, Train Loss: 0.893\n",
      "Test Loss: 0.389, Test Acc: 87.60%\n",
      "Epoch: 83, Train Loss: 0.899\n",
      "Test Loss: 0.442, Test Acc: 86.67%\n",
      "Epoch: 84, Train Loss: 0.846\n",
      "Test Loss: 0.465, Test Acc: 85.88%\n",
      "Epoch: 85, Train Loss: 0.897\n",
      "Test Loss: 0.429, Test Acc: 87.33%\n",
      "Epoch: 86, Train Loss: 0.875\n",
      "Test Loss: 0.441, Test Acc: 87.29%\n",
      "Epoch: 87, Train Loss: 0.873\n",
      "Test Loss: 0.393, Test Acc: 88.57%\n",
      "Epoch: 88, Train Loss: 0.848\n",
      "Test Loss: 0.387, Test Acc: 88.11%\n",
      "Epoch: 89, Train Loss: 0.861\n",
      "Test Loss: 0.410, Test Acc: 87.38%\n",
      "Epoch: 90, Train Loss: 0.875\n",
      "Test Loss: 0.488, Test Acc: 86.02%\n",
      "Epoch: 91, Train Loss: 0.851\n",
      "Test Loss: 0.479, Test Acc: 86.66%\n",
      "Epoch: 92, Train Loss: 0.814\n",
      "Test Loss: 0.356, Test Acc: 88.85%\n",
      "Epoch: 93, Train Loss: 0.869\n",
      "Test Loss: 0.388, Test Acc: 87.56%\n",
      "Epoch: 94, Train Loss: 0.841\n",
      "Test Loss: 0.406, Test Acc: 88.15%\n",
      "Epoch: 95, Train Loss: 0.801\n",
      "Test Loss: 0.373, Test Acc: 88.94%\n",
      "Epoch: 96, Train Loss: 0.827\n",
      "Test Loss: 0.459, Test Acc: 86.95%\n",
      "Epoch: 97, Train Loss: 0.846\n",
      "Test Loss: 0.426, Test Acc: 87.32%\n",
      "Epoch: 98, Train Loss: 0.801\n",
      "Test Loss: 0.333, Test Acc: 89.20%\n",
      "Epoch: 99, Train Loss: 0.857\n",
      "Test Loss: 0.404, Test Acc: 88.44%\n",
      "Epoch: 100, Train Loss: 0.796\n",
      "Test Loss: 0.348, Test Acc: 89.55%\n",
      "Epoch: 101, Train Loss: 0.788\n",
      "Test Loss: 0.382, Test Acc: 89.57%\n",
      "Epoch: 102, Train Loss: 0.855\n",
      "Test Loss: 0.392, Test Acc: 89.50%\n",
      "Epoch: 103, Train Loss: 0.764\n",
      "Test Loss: 0.302, Test Acc: 90.78%\n",
      "Epoch: 104, Train Loss: 0.771\n",
      "Test Loss: 0.291, Test Acc: 90.66%\n",
      "Epoch: 105, Train Loss: 0.762\n",
      "Test Loss: 0.326, Test Acc: 90.63%\n",
      "Epoch: 106, Train Loss: 0.798\n",
      "Test Loss: 0.348, Test Acc: 90.90%\n",
      "Epoch: 107, Train Loss: 0.757\n",
      "Test Loss: 0.352, Test Acc: 89.20%\n",
      "Epoch: 108, Train Loss: 0.728\n",
      "Test Loss: 0.284, Test Acc: 91.28%\n",
      "Epoch: 109, Train Loss: 0.791\n",
      "Test Loss: 0.347, Test Acc: 90.77%\n",
      "Epoch: 110, Train Loss: 0.774\n",
      "Test Loss: 0.318, Test Acc: 90.18%\n",
      "Epoch: 111, Train Loss: 0.734\n",
      "Test Loss: 0.283, Test Acc: 91.83%\n",
      "Epoch: 112, Train Loss: 0.722\n",
      "Test Loss: 0.298, Test Acc: 91.11%\n",
      "Epoch: 113, Train Loss: 0.695\n",
      "Test Loss: 0.283, Test Acc: 91.34%\n",
      "Epoch: 114, Train Loss: 0.738\n",
      "Test Loss: 0.310, Test Acc: 91.56%\n",
      "Epoch: 115, Train Loss: 0.738\n",
      "Test Loss: 0.300, Test Acc: 92.09%\n",
      "Epoch: 116, Train Loss: 0.764\n",
      "Test Loss: 0.273, Test Acc: 92.48%\n",
      "Epoch: 117, Train Loss: 0.716\n",
      "Test Loss: 0.293, Test Acc: 91.29%\n",
      "Epoch: 118, Train Loss: 0.715\n",
      "Test Loss: 0.303, Test Acc: 92.47%\n",
      "Epoch: 119, Train Loss: 0.679\n",
      "Test Loss: 0.265, Test Acc: 92.41%\n",
      "Epoch: 120, Train Loss: 0.727\n",
      "Test Loss: 0.262, Test Acc: 92.03%\n",
      "Epoch: 121, Train Loss: 0.697\n",
      "Test Loss: 0.300, Test Acc: 92.62%\n",
      "Epoch: 122, Train Loss: 0.715\n",
      "Test Loss: 0.326, Test Acc: 92.98%\n",
      "Epoch: 123, Train Loss: 0.688\n",
      "Test Loss: 0.260, Test Acc: 93.07%\n",
      "Epoch: 124, Train Loss: 0.697\n",
      "Test Loss: 0.238, Test Acc: 93.26%\n",
      "Epoch: 125, Train Loss: 0.676\n",
      "Test Loss: 0.243, Test Acc: 93.23%\n",
      "Epoch: 126, Train Loss: 0.714\n",
      "Test Loss: 0.246, Test Acc: 93.24%\n",
      "Epoch: 127, Train Loss: 0.678\n",
      "Test Loss: 0.280, Test Acc: 93.58%\n",
      "Epoch: 128, Train Loss: 0.629\n",
      "Test Loss: 0.239, Test Acc: 93.61%\n",
      "Epoch: 129, Train Loss: 0.617\n",
      "Test Loss: 0.209, Test Acc: 93.69%\n",
      "Epoch: 130, Train Loss: 0.595\n",
      "Test Loss: 0.232, Test Acc: 93.60%\n",
      "Epoch: 131, Train Loss: 0.632\n",
      "Test Loss: 0.236, Test Acc: 93.63%\n",
      "Epoch: 132, Train Loss: 0.635\n",
      "Test Loss: 0.290, Test Acc: 93.84%\n",
      "Epoch: 133, Train Loss: 0.570\n",
      "Test Loss: 0.255, Test Acc: 94.05%\n",
      "Epoch: 134, Train Loss: 0.615\n",
      "Test Loss: 0.232, Test Acc: 94.25%\n",
      "Epoch: 135, Train Loss: 0.577\n",
      "Test Loss: 0.233, Test Acc: 94.28%\n",
      "Epoch: 136, Train Loss: 0.637\n",
      "Test Loss: 0.206, Test Acc: 94.67%\n",
      "Epoch: 137, Train Loss: 0.632\n",
      "Test Loss: 0.212, Test Acc: 94.54%\n",
      "Epoch: 138, Train Loss: 0.577\n",
      "Test Loss: 0.202, Test Acc: 94.65%\n",
      "Epoch: 139, Train Loss: 0.602\n",
      "Test Loss: 0.221, Test Acc: 94.81%\n",
      "Epoch: 140, Train Loss: 0.585\n",
      "Test Loss: 0.215, Test Acc: 94.94%\n",
      "Epoch: 141, Train Loss: 0.558\n",
      "Test Loss: 0.185, Test Acc: 94.90%\n",
      "Epoch: 142, Train Loss: 0.651\n",
      "Test Loss: 0.207, Test Acc: 94.65%\n",
      "Epoch: 143, Train Loss: 0.557\n",
      "Test Loss: 0.212, Test Acc: 94.81%\n",
      "Epoch: 144, Train Loss: 0.573\n",
      "Test Loss: 0.184, Test Acc: 94.98%\n",
      "Epoch: 145, Train Loss: 0.564\n",
      "Test Loss: 0.178, Test Acc: 94.98%\n",
      "Epoch: 146, Train Loss: 0.591\n",
      "Test Loss: 0.227, Test Acc: 94.92%\n",
      "Epoch: 147, Train Loss: 0.596\n",
      "Test Loss: 0.190, Test Acc: 95.01%\n",
      "Epoch: 148, Train Loss: 0.597\n",
      "Test Loss: 0.220, Test Acc: 94.92%\n",
      "Epoch: 149, Train Loss: 0.523\n",
      "Test Loss: 0.218, Test Acc: 95.04%\n",
      "Epoch: 150, Train Loss: 0.570\n",
      "Test Loss: 0.177, Test Acc: 95.03%\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>test_accuracy</td><td>▁▂▃▅▄▅▃▅▅▆▅▆▅▆▅▆▅▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>test_loss</td><td>█▇▅▄▃▃▃▃▃▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▄▄▄▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_accuracy</td><td>95.04</td></tr><tr><td>epoch</td><td>150</td></tr><tr><td>final_accuracy</td><td>95.03</td></tr><tr><td>test_accuracy</td><td>95.03</td></tr><tr><td>test_loss</td><td>0.17746</td></tr><tr><td>train_loss</td><td>0.5703</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-sweep-1</strong> at: <a href='https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep/runs/xwg1cm8l' target=\"_blank\">https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep/runs/xwg1cm8l</a><br> View project at: <a href='https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep' target=\"_blank\">https://wandb.ai/kp2653-new-york-university/cifar10-augmentation-sweep</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250313_202547-xwg1cm8l/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
     ]
    }
   ],
   "source": [
    "# 6. Sweep Configuration and Execution\n",
    "if __name__ == '__main__':\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "    \n",
    "    wandb.login()\n",
    "    sweep_config = {\n",
    "        \"method\": \"grid\",\n",
    "        \"metric\": {\"name\": \"test_accuracy\", \"goal\": \"maximize\"},\n",
    "        \"parameters\": {\n",
    "            \"mixup_alpha\": {\"values\": [0.2]},\n",
    "            \"random_crop\": {\"values\": [True]},\n",
    "            \"horizontal_flip\": {\"values\": [True]}, #Horizontal Flip really help！\n",
    "            \"color_jitter_brightness\": {\"values\": [0.2]},\n",
    "            \"random_erasing_prob\": {\"values\": [0.2]},\n",
    "            \"num_epochs\": {\"values\": [150]}  # Add this line to set num_epochs\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"cifar10-augmentation-sweep\")\n",
    "    # wandb.agent(sweep_id, train, count=10)  # count=20 mean run 20 experiments\n",
    "    wandb.agent(sweep_id, train)  # no need of count. automatically run all the combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved.\n"
     ]
    }
   ],
   "source": [
    "# # Generate submission file\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ResNet18(num_classes=10)\n",
    "model.load_state_dict(torch.load('best_model_xwg1cm8l.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "cifar_test_path = 'deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl'\n",
    "test_batch = load_cifar_batch(cifar_test_path)\n",
    "test_images = test_batch[b'data'].astype(np.float32) / 255.0\n",
    "\n",
    "# Convert test dataset to Tensor\n",
    "test_dataset = [(test_transform(img),) for img in test_images]\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        images = batch[0].to(device)  # Get images tensor from tuple and move to device\n",
    "        outputs = model(images) \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Generate submission file\n",
    "submission = pd.DataFrame({'ID': np.arange(len(predictions)), 'Labels': predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA for acceleration\n",
      "Using best model from run major-sweep-1 with accuracy 95.04%\n",
      "Successfully loaded model from wandb\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions saved to submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Run the prediction function\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m \u001b[43mpredict_test_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 112\u001b[0m, in \u001b[0;36mpredict_test_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, indices \u001b[38;5;129;01min\u001b[39;00m test_loader:\n\u001b[1;32m    113\u001b[0m         images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    114\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(images)\n",
      "File \u001b[0;32m~/proj1/env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/proj1/env/lib64/python3.9/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/proj1/env/lib64/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/proj1/env/lib64/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m, in \u001b[0;36mCustomTestDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m---> 21\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m     23\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# 7. Prediction on Test Dataset\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb\n",
    "import os\n",
    "import glob\n",
    "\n",
    "class CustomTestDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, idx  # Return index as a placeholder for label\n",
    "\n",
    "def predict_test_dataset():\n",
    "    # Load the test dataset\n",
    "    with open('deep-learning-spring-2025-project-1/cifar_test_nolabel.pkl', 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    \n",
    "    # Define the same transformations used for testing\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465],\n",
    "            std=[0.2470, 0.2435, 0.2616]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Create test dataset and dataloader\n",
    "    test_dataset = CustomTestDataset(test_data, transform=test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "    \n",
    "    # Set device\n",
    "    if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "            print(\"Using CUDA for acceleration\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU (no GPU available)\")\n",
    "    \n",
    "    # Initialize model using the same architecture as in training\n",
    "    model = ResNet18(num_classes=10).to(device)\n",
    "    \n",
    "    # Try to find the best model from all runs\n",
    "    try:\n",
    "        api = wandb.Api()\n",
    "        runs = api.runs(\"cifar10-augmentation-sweep\")\n",
    "        best_run = None\n",
    "        best_accuracy = 0\n",
    "        \n",
    "        for run in runs:\n",
    "            if run.state == \"finished\" and run.summary.get(\"best_accuracy\", 0) > best_accuracy:\n",
    "                best_accuracy = run.summary.get(\"best_accuracy\", 0)\n",
    "                best_run = run\n",
    "        \n",
    "        if best_run:\n",
    "            run_id = best_run.id\n",
    "            best_model_path = f'best_model_{run_id}.pth'\n",
    "            print(f\"Using best model from run {best_run.name} with accuracy {best_accuracy:.2f}%\")\n",
    "            \n",
    "            try:\n",
    "                # Try to download the model file\n",
    "                model_file = best_run.file(best_model_path).download(replace=True)\n",
    "                model.load_state_dict(torch.load(model_file.name, map_location=device))\n",
    "                print(f\"Successfully loaded model from wandb\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading model from wandb: {e}\")\n",
    "                \n",
    "                # Try to find the model locally\n",
    "                if os.path.exists(best_model_path):\n",
    "                    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "                    print(f\"Loaded model from local file: {best_model_path}\")\n",
    "                else:\n",
    "                    print(f\"Could not find model file: {best_model_path}\")\n",
    "                    \n",
    "                    # Try to find any best model file locally\n",
    "                    best_models = glob.glob('best_model_*.pth')\n",
    "                    if best_models:\n",
    "                        latest_model = max(best_models, key=os.path.getctime)\n",
    "                        model.load_state_dict(torch.load(latest_model, map_location=device))\n",
    "                        print(f\"Loaded most recent local model: {latest_model}\")\n",
    "                    else:\n",
    "                        print(\"No model files found locally\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing wandb: {e}\")\n",
    "        \n",
    "        # Try to find any best model file locally\n",
    "        best_models = glob.glob('best_model_*.pth')\n",
    "        if best_models:\n",
    "            latest_model = max(best_models, key=os.path.getctime)\n",
    "            model.load_state_dict(torch.load(latest_model, map_location=device))\n",
    "            print(f\"Loaded most recent local model: {latest_model}\")\n",
    "        else:\n",
    "            print(\"WARNING: Could not load any model. Using untrained model for predictions.\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "\n",
    "# Run the prediction function\n",
    "predict_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import os\n",
    "# import glob\n",
    "# from PIL import Image\n",
    "\n",
    "# def predict_test_dataset(model_path=None):\n",
    "#     print(\"Starting prediction function...\")\n",
    "    \n",
    "#     # Load the test dataset\n",
    "#     print(\"Loading test data...\")\n",
    "#     with open('/Users/JL/Desktop/DL_Proj/cifar_test_nolabel.pkl', 'rb') as f:\n",
    "#         test_data = pickle.load(f)\n",
    "#     print(\"Test data loaded successfully!\")\n",
    "    \n",
    "#     # Define the same transformations used for testing\n",
    "#     test_transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(\n",
    "#             mean=[0.4914, 0.4822, 0.4465],\n",
    "#             std=[0.2470, 0.2435, 0.2616]\n",
    "#         )\n",
    "#     ])\n",
    "    \n",
    "#     # Create a proper dataset based on the actual structure\n",
    "#     class CIFARTestDataset(Dataset):\n",
    "#         def __init__(self, data_dict, transform=None):\n",
    "#             self.images = data_dict[b'data']  # Already in shape (10000, 32, 32, 3)\n",
    "#             self.ids = data_dict[b'ids']\n",
    "#             self.transform = transform\n",
    "            \n",
    "#         def __len__(self):\n",
    "#             return len(self.images)\n",
    "        \n",
    "#         def __getitem__(self, idx):\n",
    "#             image = self.images[idx]\n",
    "#             image_id = int(self.ids[idx])  # Convert to int for DataFrame\n",
    "            \n",
    "#             # Convert to PIL Image for transforms\n",
    "#             image = Image.fromarray(image)\n",
    "            \n",
    "#             if self.transform:\n",
    "#                 image = self.transform(image)\n",
    "                \n",
    "#             return image, image_id\n",
    "    \n",
    "#     # Create test dataset and dataloader\n",
    "#     print(\"Creating dataset and dataloader...\")\n",
    "#     test_dataset = CIFARTestDataset(test_data, transform=test_transform)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
    "#     print(f\"Dataset created with {len(test_dataset)} samples\")\n",
    "    \n",
    "#     # Set device\n",
    "#     print(\"Setting up device...\")\n",
    "#     if torch.backends.mps.is_available():\n",
    "#         device = torch.device(\"mps\")\n",
    "#         print(f\"Using Apple MPS for acceleration\")\n",
    "#     elif torch.cuda.is_available():\n",
    "#         device = torch.device(\"cuda\")\n",
    "#         print(f\"Using CUDA for acceleration\")\n",
    "#     else:\n",
    "#         device = torch.device(\"cpu\")\n",
    "#         print(f\"Using CPU (no GPU/MPS available)\")\n",
    "    \n",
    "#     # Initialize model using the same architecture as in training\n",
    "#     print(\"Initializing model...\")\n",
    "#     model = get_resnet18_model(num_classes=10).to(device)\n",
    "#     print(\"Model initialized\")\n",
    "    \n",
    "#     # Load the specified model if provided\n",
    "#     print(\"Loading model weights...\")\n",
    "#     if model_path and os.path.exists(model_path):\n",
    "#         model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "#         print(f\"Successfully loaded model from: {model_path}\")\n",
    "#     else:\n",
    "#         # If no specific model is provided, try to find the best model\n",
    "#         if model_path:\n",
    "#             print(f\"Specified model {model_path} not found.\")\n",
    "        \n",
    "#         # Try to find any best model file locally\n",
    "#         best_models = glob.glob('best_model_*.pth')\n",
    "#         if best_models:\n",
    "#             latest_model = max(best_models, key=os.path.getctime)\n",
    "#             model.load_state_dict(torch.load(latest_model, map_location=device))\n",
    "#             print(f\"Loaded most recent local model: {latest_model}\")\n",
    "#         else:\n",
    "#             print(\"WARNING: Could not load any model. Using untrained model for predictions.\")\n",
    "    \n",
    "#     # Set model to evaluation mode\n",
    "#     model.eval()\n",
    "#     print(\"Model set to evaluation mode\")\n",
    "    \n",
    "#     # Make predictions\n",
    "#     print(\"Making predictions...\")\n",
    "#     predictions = []\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (images, ids) in enumerate(test_loader):\n",
    "#             print(f\"Processing batch {batch_idx+1}/{len(test_loader)}\")\n",
    "#             images = images.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "#             for id_val, pred in zip(ids.numpy(), preds.cpu().numpy()):\n",
    "#                 predictions.append((id_val, pred))\n",
    "    \n",
    "#     # Create submission DataFrame\n",
    "#     print(\"Creating submission file...\")\n",
    "#     submission_df = pd.DataFrame(predictions, columns=['ID', 'Labels'])\n",
    "    \n",
    "#     # Save to CSV\n",
    "#     submission_df.to_csv('submission.csv', index=False)\n",
    "#     print(f\"Predictions saved to submission.csv\")\n",
    "#     print(\"Prediction function completed successfully!\")\n",
    "\n",
    "# # Call the function with your model path\n",
    "# print(\"About to call prediction function...\")\n",
    "# predict_test_dataset(\"/Users/JL/Desktop/DL_Proj/final_model_ku8uu15z.pth\")\n",
    "# print(\"Prediction function call completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# import os\n",
    "# import glob\n",
    "# from PIL import Image\n",
    "\n",
    "# def debug_test_dataset():\n",
    "#     print(\"Starting debug function...\")\n",
    "    \n",
    "#     try:\n",
    "#         # Load the test dataset\n",
    "#         print(\"Attempting to load test data...\")\n",
    "#         with open('/Users/JL/Desktop/DL_Proj/cifar_test_nolabel.pkl', 'rb') as f:\n",
    "#             test_data = pickle.load(f)\n",
    "#         print(\"Test data loaded successfully!\")\n",
    "        \n",
    "#         # Print test data structure for debugging\n",
    "#         print(f\"Test data type: {type(test_data)}\")\n",
    "        \n",
    "#         if isinstance(test_data, dict):\n",
    "#             print(f\"Test data keys: {list(test_data.keys())}\")\n",
    "#         elif isinstance(test_data, list):\n",
    "#             print(f\"Test data is a list with {len(test_data)} items\")\n",
    "#         else:\n",
    "#             print(f\"Test data is of type {type(test_data)}\")\n",
    "            \n",
    "#         print(\"Debug function completed successfully!\")\n",
    "#         return test_data\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in debug function: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return None\n",
    "\n",
    "# # Call the debug function\n",
    "# test_data = debug_test_dataset()\n",
    "\n",
    "# # If we got data, try to examine it further\n",
    "# if test_data is not None:\n",
    "#     print(\"\\nExamining test data further...\")\n",
    "#     if isinstance(test_data, dict):\n",
    "#         for key in test_data.keys():\n",
    "#             print(f\"Key: {key}, Type: {type(test_data[key])}\")\n",
    "#             if isinstance(test_data[key], (list, tuple, np.ndarray)):\n",
    "#                 print(f\"Length/Shape: {len(test_data[key]) if isinstance(test_data[key], (list, tuple)) else test_data[key].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env)",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
